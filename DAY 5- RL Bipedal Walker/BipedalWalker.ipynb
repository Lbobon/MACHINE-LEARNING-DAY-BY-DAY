{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bxhu8z6rjs32"
      },
      "source": [
        "# Necessary installation for running in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "colab_type": "code",
        "id": "H18Dq-YjR7k7",
        "outputId": "fc283fca-5b6d-41dc-a9a4-895e77a0e242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~cikit-learn (C:\\Users\\bonit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~cikit-learn (C:\\Users\\bonit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~cikit-learn (C:\\Users\\bonit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (cross-platform)\n",
        "%pip install -q \"gym==0.26.2\" \"Box2D==2.3.10\" \"pyglet==1.5.27\" \"pyopengl\" \"imageio\" \"imageio-ffmpeg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting moviepy\n",
            "  Using cached moviepy-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from moviepy) (5.2.1)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.25.0 in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from moviepy) (1.26.4)\n",
            "Collecting proglog<=1.0.0 (from moviepy)\n",
            "  Using cached proglog-0.1.12-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: python-dotenv>=0.10 in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from moviepy) (1.1.0)\n",
            "Requirement already satisfied: pillow<12.0,>=9.2.0 in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from moviepy) (10.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from proglog<=1.0.0->moviepy) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\bonit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->proglog<=1.0.0->moviepy) (0.4.6)\n",
            "Using cached moviepy-2.2.1-py3-none-any.whl (129 kB)\n",
            "Using cached proglog-0.1.12-py3-none-any.whl (6.3 kB)\n",
            "Installing collected packages: proglog, moviepy\n",
            "\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   -------------------- ------------------- 1/2 [moviepy]\n",
            "   ---------------------------------------- 2/2 [moviepy]\n",
            "\n",
            "Successfully installed moviepy-2.2.1 proglog-0.1.12\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~cikit-learn (C:\\Users\\bonit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~cikit-learn (C:\\Users\\bonit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~cikit-learn (C:\\Users\\bonit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8KjX9UtlSWKB"
      },
      "outputs": [],
      "source": [
        "# Headless support for Linux (no-op on Windows/Mac)\n",
        "import os, platform\n",
        "if platform.system() == \"Linux\" and not os.environ.get(\"DISPLAY\"):\n",
        "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q9UYPMYakrpL"
      },
      "source": [
        "# Augmented Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Azjqd6JOVcy7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gRtzFlKFVpAb"
      },
      "outputs": [],
      "source": [
        "class HP():\n",
        "    # Hyperparameters\n",
        "    def __init__(self,\n",
        "                 nb_steps=1000,\n",
        "                 episode_length=2000,\n",
        "                 learning_rate=0.02,\n",
        "                 num_deltas=16,\n",
        "                 num_best_deltas=16,\n",
        "                 noise=0.03,\n",
        "                 seed=1,\n",
        "                 env_name='BipedalWalker-v3',\n",
        "                 record_every=50,\n",
        "                 use_tanh_actions=True,\n",
        "                 rff_dim=256,\n",
        "                 rff_scale=1.0,\n",
        "                 learning_rate_decay=0.995,\n",
        "                 noise_decay=0.995,\n",
        "                 min_learning_rate=1e-4,\n",
        "                 min_noise=0.005):\n",
        "\n",
        "        self.nb_steps = nb_steps\n",
        "        self.episode_length = episode_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_deltas = num_deltas\n",
        "        self.num_best_deltas = num_best_deltas\n",
        "        assert self.num_best_deltas <= self.num_deltas\n",
        "        self.noise = noise\n",
        "        self.seed = seed\n",
        "        self.env_name = env_name\n",
        "        self.record_every = record_every\n",
        "        self.use_tanh_actions = use_tanh_actions\n",
        "        self.rff_dim = rff_dim\n",
        "        self.rff_scale = rff_scale\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "        self.noise_decay = noise_decay\n",
        "        self.min_learning_rate = min_learning_rate\n",
        "        self.min_noise = min_noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qcwrRlAlVpI8"
      },
      "outputs": [],
      "source": [
        "class Normalizer():\n",
        "    # Normalizes the inputs\n",
        "    def __init__(self, nb_inputs):\n",
        "        self.n = np.zeros(nb_inputs)\n",
        "        self.mean = np.zeros(nb_inputs)\n",
        "        self.mean_diff = np.zeros(nb_inputs)\n",
        "        self.var = np.zeros(nb_inputs)\n",
        "\n",
        "    def observe(self, x):\n",
        "        self.n += 1.0\n",
        "        last_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.n\n",
        "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
        "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        obs_mean = self.mean\n",
        "        obs_std = np.sqrt(self.var)\n",
        "        return (inputs - obs_mean) / obs_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SAakK-txVpQR"
      },
      "outputs": [],
      "source": [
        "class Policy():\n",
        "    def __init__(self, input_size, output_size, hp):\n",
        "        # Random Fourier Features for richer linear policy\n",
        "        self.hp = hp\n",
        "        self.rff_W = np.random.randn(hp.rff_dim, input_size) * (hp.rff_scale / np.sqrt(input_size))\n",
        "        self.rff_b = 2 * np.pi * np.random.rand(hp.rff_dim)\n",
        "        self.theta = np.zeros((output_size, hp.rff_dim))\n",
        "\n",
        "    def featurize(self, x):\n",
        "        z = self.rff_W.dot(x) + self.rff_b\n",
        "        return np.sqrt(2.0 / self.hp.rff_dim) * np.cos(z)\n",
        "\n",
        "    def evaluate(self, input, delta = None, direction = None):\n",
        "        phi = self.featurize(input)\n",
        "        if direction is None:\n",
        "            y = self.theta.dot(phi)\n",
        "        elif direction == \"+\":\n",
        "            y = (self.theta + self.hp.noise * delta).dot(phi)\n",
        "        elif direction == \"-\":\n",
        "            y = (self.theta - self.hp.noise * delta).dot(phi)\n",
        "        else:\n",
        "            y = self.theta.dot(phi)\n",
        "        if self.hp.use_tanh_actions:\n",
        "            y = np.tanh(y)\n",
        "        return y\n",
        "\n",
        "    def sample_deltas(self):\n",
        "        return [np.random.randn(*self.theta.shape) for _ in range(self.hp.num_deltas)]\n",
        "\n",
        "    def update(self, rollouts, sigma_rewards):\n",
        "        # sigma_rewards is the standard deviation of the rewards\n",
        "        step = np.zeros(self.theta.shape)\n",
        "        for r_pos, r_neg, delta in rollouts:\n",
        "            step += (r_pos - r_neg) * delta\n",
        "        self.theta += self.hp.learning_rate / (self.hp.num_best_deltas * (sigma_rewards + 1e-8)) * step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DgzIRF71VxNm"
      },
      "outputs": [],
      "source": [
        "class ARSTrainer():\n",
        "    def __init__(self,\n",
        "                 hp=None,\n",
        "                 input_size=None,\n",
        "                 output_size=None,\n",
        "                 normalizer=None,\n",
        "                 policy=None,\n",
        "                 monitor_dir=None):\n",
        "\n",
        "        self.hp = hp or HP()\n",
        "        np.random.seed(self.hp.seed)\n",
        "        # Create env with render_mode for video compatibility (Gym 0.26+)\n",
        "        try:\n",
        "            self.env = gym.make(self.hp.env_name, render_mode=\"rgb_array\")\n",
        "        except TypeError:\n",
        "            # Older Gym\n",
        "            self.env = gym.make(self.hp.env_name)\n",
        "        if monitor_dir is not None:\n",
        "            # Prefer RecordVideo (Gym 0.26+)\n",
        "            try:\n",
        "                from gym.wrappers import RecordVideo\n",
        "                episode_trigger = lambda episode_id: self.record_video\n",
        "                self.env = RecordVideo(self.env, video_folder=monitor_dir, episode_trigger=episode_trigger)\n",
        "            except Exception:\n",
        "                # Fallback to older Monitor API if available\n",
        "                try:\n",
        "                    from gym import wrappers as gym_wrappers\n",
        "                    should_record = lambda i: self.record_video\n",
        "                    self.env = gym_wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        # Set episode length from env if available\n",
        "        max_steps = None\n",
        "        if hasattr(self.env, \"spec\") and getattr(self.env.spec, \"max_episode_steps\", None) is not None:\n",
        "            max_steps = self.env.spec.max_episode_steps\n",
        "        elif hasattr(self.env, \"_max_episode_steps\"):\n",
        "            max_steps = self.env._max_episode_steps\n",
        "        self.hp.episode_length = max_steps or self.hp.episode_length\n",
        "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
        "        self.output_size = output_size or self.env.action_space.shape[0]\n",
        "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
        "        self.policy = policy or Policy(self.input_size, self.output_size, self.hp)\n",
        "        self.record_video = False\n",
        "        self.best_theta = None\n",
        "        self.best_eval_reward = -np.inf\n",
        "\n",
        "    # Explore the policy on one specific direction and over one episode\n",
        "    def explore(self, direction=None, delta=None):\n",
        "        state = self.env.reset()\n",
        "        # Gym 0.26+ returns (obs, info)\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]\n",
        "        done = False\n",
        "        num_plays = 0.0\n",
        "        sum_rewards = 0.0\n",
        "        while not done and num_plays < self.hp.episode_length:\n",
        "            self.normalizer.observe(state)\n",
        "            state = self.normalizer.normalize(state)\n",
        "            action = self.policy.evaluate(state, delta, direction)\n",
        "            # Clip to action space bounds when available\n",
        "            try:\n",
        "                action = np.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
        "            except Exception:\n",
        "                pass\n",
        "            step_result = self.env.step(action)\n",
        "            if len(step_result) == 5:\n",
        "                state, reward, terminated, truncated, _ = step_result\n",
        "                done = terminated or truncated\n",
        "            else:\n",
        "                state, reward, done, _ = step_result\n",
        "            # Do not clip reward; preserve informative signals\n",
        "            sum_rewards += float(reward)\n",
        "            num_plays += 1\n",
        "        return sum_rewards\n",
        "\n",
        "    def train(self):\n",
        "        for step in range(self.hp.nb_steps):\n",
        "            # initialize the random noise deltas and the positive/negative rewards\n",
        "            deltas = self.policy.sample_deltas()\n",
        "            positive_rewards = [0] * self.hp.num_deltas\n",
        "            negative_rewards = [0] * self.hp.num_deltas\n",
        "\n",
        "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
        "            for k in range(self.hp.num_deltas):\n",
        "                positive_rewards[k] = self.explore(direction=\"+\", delta=deltas[k])\n",
        "                negative_rewards[k] = self.explore(direction=\"-\", delta=deltas[k])\n",
        "                \n",
        "            # Compute the standard deviation of all rewards\n",
        "            sigma_rewards = np.array(positive_rewards + negative_rewards).std() + 1e-8\n",
        "\n",
        "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
        "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
        "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.hp.num_best_deltas]\n",
        "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
        "\n",
        "            # Update the policy\n",
        "            self.policy.update(rollouts, sigma_rewards)\n",
        "\n",
        "            # Decay learning rate and noise (without extending steps)\n",
        "            self.hp.learning_rate = max(self.hp.min_learning_rate, self.hp.learning_rate * self.hp.learning_rate_decay)\n",
        "            self.hp.noise = max(self.hp.min_noise, self.hp.noise * self.hp.noise_decay)\n",
        "\n",
        "            # Only record video during evaluation, every n steps\n",
        "            if step % self.hp.record_every == 0:\n",
        "                self.record_video = True\n",
        "            # Play an episode with the new weights and print the score\n",
        "            reward_evaluation = self.explore()\n",
        "            print('Step: ', step, 'Reward: ', reward_evaluation)\n",
        "            self.record_video = False\n",
        "\n",
        "            # Track best policy and keep it if improved\n",
        "            if reward_evaluation > self.best_eval_reward:\n",
        "                self.best_eval_reward = reward_evaluation\n",
        "                self.best_theta = self.policy.theta.copy()\n",
        "\n",
        "        # Restore best policy at the end\n",
        "        if self.best_theta is not None:\n",
        "            self.policy.theta = self.best_theta.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "p2aIv0lMV3_F"
      },
      "outputs": [],
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17051
        },
        "colab_type": "code",
        "id": "IVuDOFFVV9xZ",
        "outputId": "c3910d47-bf2a-4e77-ff44-2a880b50dfcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Building video c:\\Users\\bonit\\Downloads\\MACHINE LEARNING DAY BY DAY\\DAY 5- RL Bipedal Walker\\videos\\BipedalWalker-v3\\rl-video-episode-32.mp4.\n",
            "MoviePy - Writing video c:\\Users\\bonit\\Downloads\\MACHINE LEARNING DAY BY DAY\\DAY 5- RL Bipedal Walker\\videos\\BipedalWalker-v3\\rl-video-episode-32.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done !\n",
            "MoviePy - video ready c:\\Users\\bonit\\Downloads\\MACHINE LEARNING DAY BY DAY\\DAY 5- RL Bipedal Walker\\videos\\BipedalWalker-v3\\rl-video-episode-32.mp4\n",
            "Step:  0 Reward:  -93.01364912948547\n",
            "Step:  1 Reward:  -92.8513136246571\n",
            "Step:  2 Reward:  -92.71392349333215\n",
            "Step:  3 Reward:  -93.04321505908383\n",
            "Step:  4 Reward:  -92.32015878345558\n",
            "Step:  5 Reward:  -93.5140540862218\n",
            "Step:  6 Reward:  -92.86960345481216\n",
            "Step:  7 Reward:  -92.01505805020305\n",
            "Step:  8 Reward:  -91.99871290152512\n",
            "Step:  9 Reward:  -11.133469509041973\n",
            "Step:  10 Reward:  -13.26068095742212\n",
            "Step:  11 Reward:  -11.156611242381485\n",
            "Step:  12 Reward:  -4.825312277134151\n",
            "Step:  13 Reward:  -12.467204531119938\n",
            "Step:  14 Reward:  -11.291375852246665\n",
            "Step:  15 Reward:  -11.449609151715567\n",
            "Step:  16 Reward:  -10.433582417174813\n",
            "Step:  17 Reward:  -5.637453716506404\n",
            "Step:  18 Reward:  -5.749648916964636\n",
            "Step:  19 Reward:  -7.075518985324694\n",
            "Step:  20 Reward:  -11.6282785642106\n",
            "Step:  21 Reward:  -7.840517091489374\n",
            "Step:  22 Reward:  -13.559910633435884\n",
            "Step:  23 Reward:  -10.961316114130762\n",
            "Step:  24 Reward:  -6.330223240161266\n",
            "Step:  25 Reward:  -10.319100712541603\n",
            "Step:  26 Reward:  -9.802552570270024\n",
            "Step:  27 Reward:  -0.6824896547215686\n",
            "Step:  28 Reward:  -8.670022981444752\n",
            "Step:  29 Reward:  -4.552196258889667\n",
            "Step:  30 Reward:  -2.3499710803729252\n",
            "Step:  31 Reward:  -0.7133043116213454\n",
            "Step:  32 Reward:  0.09764618749681953\n",
            "Step:  33 Reward:  -0.9342467412003169\n",
            "Step:  34 Reward:  -0.9773715066254788\n",
            "Step:  35 Reward:  0.6705113835697095\n",
            "Step:  36 Reward:  -5.754395036535265\n",
            "Step:  37 Reward:  -5.6918244215320035\n",
            "Step:  38 Reward:  -2.9687968832011102\n",
            "Step:  39 Reward:  -6.309409317668225\n",
            "Step:  40 Reward:  -2.2422216426463164\n",
            "Step:  41 Reward:  -12.675482027640284\n",
            "Step:  42 Reward:  -10.603384513670882\n",
            "Step:  43 Reward:  -11.479918825455115\n",
            "Step:  44 Reward:  -7.691539769531505\n",
            "Step:  45 Reward:  -8.391473886266372\n",
            "Step:  46 Reward:  -13.478914174765437\n",
            "Step:  47 Reward:  4.213347159628608\n",
            "Step:  48 Reward:  3.7650512491050097\n",
            "Step:  49 Reward:  3.0772598016984762\n",
            "MoviePy - Building video c:\\Users\\bonit\\Downloads\\MACHINE LEARNING DAY BY DAY\\DAY 5- RL Bipedal Walker\\videos\\BipedalWalker-v3\\rl-video-episode-1682.mp4.\n",
            "MoviePy - Writing video c:\\Users\\bonit\\Downloads\\MACHINE LEARNING DAY BY DAY\\DAY 5- RL Bipedal Walker\\videos\\BipedalWalker-v3\\rl-video-episode-1682.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done !\n",
            "MoviePy - video ready c:\\Users\\bonit\\Downloads\\MACHINE LEARNING DAY BY DAY\\DAY 5- RL Bipedal Walker\\videos\\BipedalWalker-v3\\rl-video-episode-1682.mp4\n",
            "Step:  50 Reward:  3.0628345076946246\n",
            "Step:  51 Reward:  6.311172832817678\n",
            "Step:  52 Reward:  3.00887079479312\n",
            "Step:  53 Reward:  4.772360242036146\n",
            "Step:  54 Reward:  5.646951315868726\n",
            "Step:  55 Reward:  6.190813795889733\n",
            "Step:  56 Reward:  4.330555365597287\n",
            "Step:  57 Reward:  6.071632907145179\n",
            "Step:  58 Reward:  4.383939051223819\n",
            "Step:  59 Reward:  6.516155554703882\n",
            "Step:  60 Reward:  3.8239836948532075\n",
            "Step:  61 Reward:  6.125271852059448\n",
            "Step:  62 Reward:  6.1711947296721466\n",
            "Step:  63 Reward:  6.361444179465918\n",
            "Step:  64 Reward:  4.237889190271267\n",
            "Step:  65 Reward:  6.692508226099239\n",
            "Step:  66 Reward:  5.251593937091923\n",
            "Step:  67 Reward:  4.959414200036497\n",
            "Step:  68 Reward:  6.286100222510372\n",
            "Step:  69 Reward:  5.258159806959852\n",
            "Step:  70 Reward:  4.904990620928347\n",
            "Step:  71 Reward:  5.097261970667521\n",
            "Step:  72 Reward:  6.237248377014562\n",
            "Step:  73 Reward:  6.10813633405996\n",
            "Step:  74 Reward:  6.1263588064376515\n",
            "Step:  75 Reward:  6.855838862182831\n",
            "Step:  76 Reward:  6.323390340101773\n",
            "Step:  77 Reward:  6.648701315060079\n",
            "Step:  78 Reward:  6.14338146681413\n",
            "Step:  79 Reward:  6.493813250625957\n",
            "Step:  80 Reward:  5.962967441495074\n",
            "Step:  81 Reward:  6.7065282667191966\n",
            "Step:  82 Reward:  6.119803482993361\n",
            "Step:  83 Reward:  6.925290649380383\n",
            "Step:  84 Reward:  6.420860158327284\n",
            "Step:  85 Reward:  7.101164634426605\n",
            "Step:  86 Reward:  6.351875589067997\n",
            "Step:  87 Reward:  6.595342296617527\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m hp = HP(env_name=ENV_NAME)\n\u001b[32m      7\u001b[39m trainer = ARSTrainer(hp=hp, monitor_dir=monitor_dir)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mARSTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.hp.num_deltas):\n\u001b[32m     85\u001b[39m     positive_rewards[k] = \u001b[38;5;28mself\u001b[39m.explore(direction=\u001b[33m\"\u001b[39m\u001b[33m+\u001b[39m\u001b[33m\"\u001b[39m, delta=deltas[k])\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     negative_rewards[k] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Compute the standard deviation of all rewards\u001b[39;00m\n\u001b[32m     89\u001b[39m sigma_rewards = np.array(positive_rewards + negative_rewards).std() + \u001b[32m1e-8\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mARSTrainer.explore\u001b[39m\u001b[34m(self, direction, delta)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m.normalizer.observe(state)\n\u001b[32m     58\u001b[39m state = \u001b[38;5;28mself\u001b[39m.normalizer.normalize(state)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Clip to action space bounds when available\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mPolicy.evaluate\u001b[39m\u001b[34m(self, input, delta, direction)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, delta = \u001b[38;5;28;01mNone\u001b[39;00m, direction = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     phi = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     16\u001b[39m         y = \u001b[38;5;28mself\u001b[39m.theta.dot(phi)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mPolicy.featurize\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeaturize\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     z = \u001b[38;5;28mself\u001b[39m.rff_W.dot(x) + \u001b[38;5;28mself\u001b[39m.rff_b\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(\u001b[32m2.0\u001b[39m / \u001b[38;5;28mself\u001b[39m.hp.rff_dim) * np.cos(z)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "ENV_NAME = 'BipedalWalker-v3'\n",
        "\n",
        "videos_dir = mkdir('.', 'videos')\n",
        "monitor_dir = mkdir(videos_dir, ENV_NAME)\n",
        "\n",
        "hp = HP(env_name=ENV_NAME)\n",
        "trainer = ARSTrainer(hp=hp, monitor_dir=monitor_dir)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4C_waL3_lKXu"
      },
      "source": [
        "# Download the episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "colab_type": "code",
        "id": "5xvR-vXbBLB1",
        "outputId": "4e8c7268-79a5-4313-840b-1a1dacdeb389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rl-video-episode-11582.meta.json\n",
            "rl-video-episode-11582.mp4\n",
            "rl-video-episode-13232.meta.json\n",
            "rl-video-episode-13232.mp4\n",
            "rl-video-episode-14882.meta.json\n",
            "rl-video-episode-14882.mp4\n",
            "rl-video-episode-16532.meta.json\n",
            "rl-video-episode-16532.mp4\n",
            "rl-video-episode-1682.meta.json\n",
            "rl-video-episode-1682.mp4\n",
            "rl-video-episode-32.meta.json\n",
            "rl-video-episode-32.mp4\n",
            "rl-video-episode-3332.meta.json\n",
            "rl-video-episode-3332.mp4\n",
            "rl-video-episode-4982.meta.json\n",
            "rl-video-episode-4982.mp4\n",
            "rl-video-episode-6632.meta.json\n",
            "rl-video-episode-6632.mp4\n",
            "rl-video-episode-8282.meta.json\n",
            "rl-video-episode-8282.mp4\n",
            "rl-video-episode-9932.meta.json\n",
            "rl-video-episode-9932.mp4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "folder = Path('videos') / ENV_NAME\n",
        "if folder.exists():\n",
        "    print('\\n'.join(sorted(p.name for p in folder.iterdir())))\n",
        "else:\n",
        "    print(f'No video folder at {folder}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MYitauj1SePX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rl-video-episode-11582.meta.json\n",
            "rl-video-episode-11582.mp4\n",
            "rl-video-episode-13232.meta.json\n",
            "rl-video-episode-13232.mp4\n",
            "rl-video-episode-14882.meta.json\n",
            "rl-video-episode-14882.mp4\n",
            "rl-video-episode-16532.meta.json\n",
            "rl-video-episode-16532.mp4\n",
            "rl-video-episode-1682.meta.json\n",
            "rl-video-episode-1682.mp4\n"
          ]
        }
      ],
      "source": [
        "# Videos are saved locally under videos/<ENV_NAME>. Open the folder to view them.\n",
        "# Example: print first few files if present.\n",
        "import os, itertools\n",
        "video_dir = os.path.join('videos', ENV_NAME)\n",
        "if os.path.isdir(video_dir):\n",
        "    for name in itertools.islice(sorted(os.listdir(video_dir)), 10):\n",
        "        print(name)\n",
        "else:\n",
        "    print(f\"No local video directory found at {video_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BipedalWalker",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
