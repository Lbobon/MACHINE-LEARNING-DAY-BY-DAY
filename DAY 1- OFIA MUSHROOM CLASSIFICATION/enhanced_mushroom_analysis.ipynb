{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Mushroom Toxicity Prediction with Probability Features\n",
    "\n",
    "Based on your analysis findings, this notebook creates enhanced features and gives more importance to highly indicative characteristics:\n",
    "\n",
    "## Key Findings to Implement:\n",
    "- **Location patterns**: Non-toxic cluster at (200,200), toxic at (0,0)\n",
    "- **pH patterns**: Toxic more distributed for pH_jus (3-6, 8-11), pH_sol (0-4.5, 8-12)\n",
    "- **Soil type**: More water type for non-toxic, more terre type for toxic\n",
    "- **Insect presence**: +15% toxicity when insects present\n",
    "- **Touch effect**: Picottement particularly indicative of toxicity\n",
    "- **Visual features**: unique_colors, brightness, rouge correlate with toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved train_img_dir: OFIA_2025_epreuve_1\\images\\train\n",
      "Resolved test_img_dir:  OFIA_2025_epreuve_1\\images\\test\n",
      "Train images directory exists: True | PNGs: 10000\n",
      "Test images directory exists:  True  | PNGs: 5000\n"
     ]
    }
   ],
   "source": [
    "# Check image directories (robust resolution)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    # 1) Preferred default: images/train or images/test\n",
    "    default = Path('images') / kind\n",
    "    if default.exists():\n",
    "        return str(default)\n",
    "\n",
    "    # 2) Any \".../images/<kind>\" path containing PNGs\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "    # 3) Any directory named <kind> that contains PNGs\n",
    "    for candidate in Path('.').rglob(kind):\n",
    "        if candidate.is_dir() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Resolved train_img_dir: {train_img_dir}\")\n",
    "print(f\"Resolved test_img_dir:  {test_img_dir}\")\n",
    "\n",
    "# Quick counts\n",
    "def count_pngs(d):\n",
    "    p = Path(d)\n",
    "    return len(list(p.glob('*.png'))) if p.exists() else 0\n",
    "\n",
    "print(f\"Train images directory exists: {Path(train_img_dir).exists()} | PNGs: {count_pngs(train_img_dir)}\")\n",
    "print(f\"Test images directory exists:  {Path(test_img_dir).exists()}  | PNGs: {count_pngs(test_img_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Enhanced libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with robust path resolution\n",
    "def resolve_csv(filename: str) -> Path:\n",
    "    p = Path(filename)\n",
    "    if p.exists():\n",
    "        return p\n",
    "    matches = list(Path('.').rglob(filename))\n",
    "    if matches:\n",
    "        print(f\"Found {filename} at: {matches[0]}\")\n",
    "        return matches[0]\n",
    "    raise FileNotFoundError(f\"Could not find {filename}\")\n",
    "\n",
    "# Load datasets\n",
    "X_train = pd.read_csv(resolve_csv('X_train.csv'))\n",
    "y_train = pd.read_csv(resolve_csv('y_train.csv'))\n",
    "X_test = pd.read_csv(resolve_csv('X_test.csv'))\n",
    "\n",
    "# Merge training data\n",
    "train_data = pd.merge(X_train, y_train, on='id')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Toxicity distribution: {train_data['est_toxique'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Feature Engineering Based on Your Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features(df):\n",
    "    \"\"\"Create enhanced features based on analysis findings\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # 1. LOCATION-BASED TOXICITY PROBABILITY\n",
    "    # Distance from non-toxic cluster (200, 200)\n",
    "    df_enhanced['dist_from_nontoxic_center'] = np.sqrt((df['x'] - 200)**2 + (df['y'] - 200)**2)\n",
    "    \n",
    "    # Distance from toxic cluster (0, 0)\n",
    "    df_enhanced['dist_from_toxic_center'] = np.sqrt((df['x'] - 0)**2 + (df['y'] - 0)**2)\n",
    "    \n",
    "    # Location-based toxicity probability\n",
    "    df_enhanced['location_toxicity_prob'] = (\n",
    "        df_enhanced['dist_from_nontoxic_center'] / \n",
    "        (df_enhanced['dist_from_nontoxic_center'] + df_enhanced['dist_from_toxic_center'] + 1e-8)\n",
    "    )\n",
    "    \n",
    "    # 2. pH-BASED TOXICITY INDICATORS\n",
    "    # pH_jus toxicity zones (3-6, 8-11 are more toxic)\n",
    "    df_enhanced['ph_jus_toxic_zone'] = (\n",
    "        ((df['ph_du_jus'] >= 3) & (df['ph_du_jus'] <= 6)) |\n",
    "        ((df['ph_du_jus'] >= 8) & (df['ph_du_jus'] <= 11))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # pH_sol toxicity zones (0-4.5, 8-12 are more toxic)\n",
    "    df_enhanced['ph_sol_toxic_zone'] = (\n",
    "        ((df['ph_du_sol'] >= 0) & (df['ph_du_sol'] <= 4.5)) |\n",
    "        ((df['ph_du_sol'] >= 8) & (df['ph_du_sol'] <= 12))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Combined pH toxicity score\n",
    "    df_enhanced['ph_toxicity_score'] = (\n",
    "        df_enhanced['ph_jus_toxic_zone'] + df_enhanced['ph_sol_toxic_zone']\n",
    "    )\n",
    "    \n",
    "    # 3. SOIL TYPE TOXICITY PROBABILITY\n",
    "    # Based on your finding: more eau for non-toxic, more terre for toxic\n",
    "    soil_toxicity_map = {\n",
    "        'eau': 0.268,    # Lower toxicity\n",
    "        'arbre': 0.335,\n",
    "        'roche': 0.375,\n",
    "        'terre': 0.420   # Higher toxicity\n",
    "    }\n",
    "    df_enhanced['soil_toxicity_prob'] = df['type_sol'].map(soil_toxicity_map)\n",
    "    \n",
    "    # 4. INSECT PRESENCE ENHANCED WEIGHT\n",
    "    # +15% toxicity when insects present\n",
    "    df_enhanced['insect_toxicity_boost'] = (df['presence_insecte'] == 'oui').astype(int) * 0.15\n",
    "    \n",
    "    # 5. TOUCH EFFECT TOXICITY INDICATOR\n",
    "    # Picottement is particularly indicative\n",
    "    touch_toxicity_map = {\n",
    "        'aucun': 0.329,\n",
    "        'brûlure': 0.305,\n",
    "        'irritation': 0.328,\n",
    "        'picottement': 0.434  # Highly indicative\n",
    "    }\n",
    "    df_enhanced['touch_toxicity_prob'] = df['effet_au_toucher'].map(touch_toxicity_map)\n",
    "    \n",
    "    # 6. WEIGHT-POROSITY SIMILARITY INDICATOR\n",
    "    # Normalized weight and porosity for comparison\n",
    "    weight_norm = (df['poids'] - df['poids'].min()) / (df['poids'].max() - df['poids'].min())\n",
    "    porosity_norm = (df['porosite'] - df['porosite'].min()) / (df['porosite'].max() - df['porosite'].min())\n",
    "    df_enhanced['weight_porosity_similarity'] = 1 - abs(weight_norm - porosity_norm)\n",
    "    \n",
    "    # 7. COMPREHENSIVE TOXICITY PROBABILITY SCORE\n",
    "    # Weighted combination of all indicators\n",
    "    df_enhanced['probability_of_toxicity'] = (\n",
    "        0.25 * df_enhanced['location_toxicity_prob'] +\n",
    "        0.20 * (df_enhanced['ph_toxicity_score'] / 2.0) +\n",
    "        0.20 * df_enhanced['soil_toxicity_prob'] +\n",
    "        0.15 * df_enhanced['insect_toxicity_boost'] +\n",
    "        0.15 * df_enhanced['touch_toxicity_prob'] +\n",
    "        0.05 * df_enhanced['weight_porosity_similarity']\n",
    "    )\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    df_enhanced['probability_of_toxicity'] = (\n",
    "        (df_enhanced['probability_of_toxicity'] - df_enhanced['probability_of_toxicity'].min()) /\n",
    "        (df_enhanced['probability_of_toxicity'].max() - df_enhanced['probability_of_toxicity'].min())\n",
    "    )\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Apply enhanced feature engineering\n",
    "train_enhanced = create_enhanced_features(train_data)\n",
    "test_enhanced = create_enhanced_features(X_test)\n",
    "\n",
    "print(f\"Enhanced training data shape: {train_enhanced.shape}\")\n",
    "print(f\"New features created: {train_enhanced.shape[1] - train_data.shape[1]}\")\n",
    "print(f\"\\nNew feature columns:\")\n",
    "new_cols = [col for col in train_enhanced.columns if col not in train_data.columns]\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the probability of toxicity feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribution of probability by actual toxicity\n",
    "plt.subplot(2, 3, 1)\n",
    "train_enhanced[train_enhanced['est_toxique']==0]['probability_of_toxicity'].hist(\n",
    "    alpha=0.7, label='Non-toxic', bins=50, density=True)\n",
    "train_enhanced[train_enhanced['est_toxique']==1]['probability_of_toxicity'].hist(\n",
    "    alpha=0.7, label='Toxic', bins=50, density=True)\n",
    "plt.xlabel('Probability of Toxicity')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Toxicity Probability')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation with actual toxicity\n",
    "plt.subplot(2, 3, 2)\n",
    "correlation = train_enhanced['probability_of_toxicity'].corr(train_enhanced['est_toxique'])\n",
    "plt.scatter(train_enhanced['probability_of_toxicity'], train_enhanced['est_toxique'], alpha=0.5)\n",
    "plt.xlabel('Probability of Toxicity')\n",
    "plt.ylabel('Actual Toxicity')\n",
    "plt.title(f'Correlation: {correlation:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Location visualization\n",
    "plt.subplot(2, 3, 3)\n",
    "scatter = plt.scatter(train_enhanced['x'], train_enhanced['y'], \n",
    "                     c=train_enhanced['probability_of_toxicity'], \n",
    "                     alpha=0.6, cmap='RdYlBu_r')\n",
    "plt.colorbar(scatter, label='Toxicity Probability')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.title('Location vs Toxicity Probability')\n",
    "\n",
    "# pH zones visualization\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(train_enhanced['ph_du_jus'], train_enhanced['ph_du_sol'], \n",
    "           c=train_enhanced['ph_toxicity_score'], alpha=0.6, cmap='RdYlBu_r')\n",
    "plt.colorbar(label='pH Toxicity Score')\n",
    "plt.xlabel('pH du jus')\n",
    "plt.ylabel('pH du sol')\n",
    "plt.title('pH Zones and Toxicity')\n",
    "\n",
    "# Feature importance comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "feature_corrs = [\n",
    "    ('location_toxicity_prob', train_enhanced['location_toxicity_prob'].corr(train_enhanced['est_toxique'])),\n",
    "    ('ph_toxicity_score', train_enhanced['ph_toxicity_score'].corr(train_enhanced['est_toxique'])),\n",
    "    ('soil_toxicity_prob', train_enhanced['soil_toxicity_prob'].corr(train_enhanced['est_toxique'])),\n",
    "    ('touch_toxicity_prob', train_enhanced['touch_toxicity_prob'].corr(train_enhanced['est_toxique'])),\n",
    "    ('probability_of_toxicity', correlation)\n",
    "]\n",
    "\n",
    "features, corrs = zip(*feature_corrs)\n",
    "colors = ['red' if abs(c) > 0.1 else 'orange' if abs(c) > 0.05 else 'gray' for c in corrs]\n",
    "plt.barh(range(len(features)), [abs(c) for c in corrs], color=colors)\n",
    "plt.yticks(range(len(features)), features, rotation=0)\n",
    "plt.xlabel('Absolute Correlation with Toxicity')\n",
    "plt.title('Enhanced Feature Correlations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC-like analysis\n",
    "plt.subplot(2, 3, 6)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "tpr_scores = []\n",
    "fpr_scores = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    predictions = (train_enhanced['probability_of_toxicity'] > thresh).astype(int)\n",
    "    tp = ((predictions == 1) & (train_enhanced['est_toxique'] == 1)).sum()\n",
    "    tn = ((predictions == 0) & (train_enhanced['est_toxique'] == 0)).sum()\n",
    "    fp = ((predictions == 1) & (train_enhanced['est_toxique'] == 0)).sum()\n",
    "    fn = ((predictions == 0) & (train_enhanced['est_toxique'] == 1)).sum()\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    tpr_scores.append(tpr)\n",
    "    fpr_scores.append(fpr)\n",
    "\n",
    "plt.plot(fpr_scores, tpr_scores, 'b-', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Toxicity Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nToxicity Probability Feature Validation:\")\n",
    "print(f\"Correlation with actual toxicity: {correlation:.4f}\")\n",
    "print(f\"Mean probability for toxic samples: {train_enhanced[train_enhanced['est_toxique']==1]['probability_of_toxicity'].mean():.4f}\")\n",
    "print(f\"Mean probability for non-toxic samples: {train_enhanced[train_enhanced['est_toxique']==0]['probability_of_toxicity'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Visual Feature Extraction with Focus on Key Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced image feature extraction focusing on key visual indicators\n",
    "def extract_enhanced_image_features(image_path):\n",
    "    \"\"\"Extract visual features with emphasis on toxicity indicators\"\"\"\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    height, width, channels = img.shape\n",
    "    area = height * width\n",
    "    \n",
    "    # Enhanced color analysis\n",
    "    mean_r = np.mean(img_rgb[:, :, 0])\n",
    "    mean_g = np.mean(img_rgb[:, :, 1])\n",
    "    mean_b = np.mean(img_rgb[:, :, 2])\n",
    "    \n",
    "    std_r = np.std(img_rgb[:, :, 0])\n",
    "    std_g = np.std(img_rgb[:, :, 1])\n",
    "    std_b = np.std(img_rgb[:, :, 2])\n",
    "    \n",
    "    # HSV analysis\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mean_hue = np.mean(img_hsv[:, :, 0])\n",
    "    mean_saturation = np.mean(img_hsv[:, :, 1])\n",
    "    mean_value = np.mean(img_hsv[:, :, 2])\n",
    "    \n",
    "    # Enhanced texture analysis\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    \n",
    "    # Multiple edge detection methods\n",
    "    edges_canny = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = np.sum(edges_canny) / area\n",
    "    \n",
    "    # Sobel edge detection for more texture info\n",
    "    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    sobel_mean = np.mean(sobel_magnitude)\n",
    "    \n",
    "    # Brightness analysis\n",
    "    brightness = np.mean(gray)\n",
    "    brightness_std = np.std(gray)\n",
    "    \n",
    "    # Enhanced color quantization\n",
    "    quantized = img_rgb // 16 * 16  # More fine-grained quantization\n",
    "    unique_colors = len(np.unique(quantized.reshape(-1, 3), axis=0))\n",
    "    \n",
    "    # Color dominance analysis\n",
    "    red_dominance = mean_r / (mean_r + mean_g + mean_b + 1e-8)\n",
    "    green_dominance = mean_g / (mean_r + mean_g + mean_b + 1e-8)\n",
    "    blue_dominance = mean_b / (mean_r + mean_g + mean_b + 1e-8)\n",
    "    \n",
    "    # Color variance (indicator of color complexity)\n",
    "    color_variance = np.var([mean_r, mean_g, mean_b])\n",
    "    \n",
    "    # Histogram analysis for color distribution\n",
    "    hist_r = cv2.calcHist([img_rgb], [0], None, [256], [0, 256])\n",
    "    hist_g = cv2.calcHist([img_rgb], [1], None, [256], [0, 256])\n",
    "    hist_b = cv2.calcHist([img_rgb], [2], None, [256], [0, 256])\n",
    "    \n",
    "    # Entropy as measure of color complexity\n",
    "    def calculate_entropy(hist):\n",
    "        hist_norm = hist.flatten() / np.sum(hist)\n",
    "        hist_norm = hist_norm[hist_norm > 0]  # Remove zeros\n",
    "        return -np.sum(hist_norm * np.log2(hist_norm))\n",
    "    \n",
    "    color_entropy = (calculate_entropy(hist_r) + calculate_entropy(hist_g) + calculate_entropy(hist_b)) / 3\n",
    "    \n",
    "    # ENHANCED FEATURES BASED ON YOUR FINDINGS\n",
    "    # Focus on unique_colors, brightness, and red channel\n",
    "    \n",
    "    # Red channel toxicity indicator (based on your finding)\n",
    "    red_toxicity_score = (mean_r - 85.72) / 35.53  # Normalized based on sample statistics\n",
    "    \n",
    "    # Brightness toxicity indicator\n",
    "    brightness_toxicity_score = abs(brightness - 134.17) / 9.22  # Distance from mean\n",
    "    \n",
    "    # Unique colors toxicity indicator\n",
    "    unique_colors_toxicity_score = (unique_colors - 5.57) / 0.66  # Normalized\n",
    "    \n",
    "    # Combined visual toxicity probability\n",
    "    visual_toxicity_prob = (\n",
    "        0.4 * (unique_colors_toxicity_score + 3) / 6 +  # Normalize to [0,1] range\n",
    "        0.35 * (brightness_toxicity_score) / 5 +\n",
    "        0.25 * (red_toxicity_score + 3) / 6\n",
    "    )\n",
    "    visual_toxicity_prob = np.clip(visual_toxicity_prob, 0, 1)  # Ensure [0,1] range\n",
    "    \n",
    "    features = {\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'area': area,\n",
    "        'aspect_ratio': width / height,\n",
    "        'mean_red': mean_r,\n",
    "        'mean_green': mean_g,\n",
    "        'mean_blue': mean_b,\n",
    "        'std_red': std_r,\n",
    "        'std_green': std_g,\n",
    "        'std_blue': std_b,\n",
    "        'mean_hue': mean_hue,\n",
    "        'mean_saturation': mean_saturation,\n",
    "        'mean_value': mean_value,\n",
    "        'brightness': brightness,\n",
    "        'brightness_std': brightness_std,\n",
    "        'texture_laplacian': laplacian_var,\n",
    "        'edge_density': edge_density,\n",
    "        'sobel_magnitude': sobel_mean,\n",
    "        'unique_colors': unique_colors,\n",
    "        'red_dominance': red_dominance,\n",
    "        'green_dominance': green_dominance,\n",
    "        'blue_dominance': blue_dominance,\n",
    "        'color_variance': color_variance,\n",
    "        'color_entropy': color_entropy,\n",
    "        'red_toxicity_score': red_toxicity_score,\n",
    "        'brightness_toxicity_score': brightness_toxicity_score,\n",
    "        'unique_colors_toxicity_score': unique_colors_toxicity_score,\n",
    "        'visual_toxicity_prob': visual_toxicity_prob\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Enhanced image feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve image directories\n",
    "def resolve_image_dir(kind: str) -> str:\n",
    "    for images_dir in Path('.').rglob('images'):\n",
    "        candidate = images_dir / kind\n",
    "        if candidate.exists() and any(candidate.glob('*.png')):\n",
    "            return str(candidate)\n",
    "    raise FileNotFoundError(f\"Could not find images/{kind} directory\")\n",
    "\n",
    "train_img_dir = resolve_image_dir('train')\n",
    "test_img_dir = resolve_image_dir('test')\n",
    "\n",
    "print(f\"Train images directory: {train_img_dir}\")\n",
    "print(f\"Test images directory: {test_img_dir}\")\n",
    "\n",
    "# Extract enhanced visual features from a larger sample\n",
    "sample_size = min(2000, len(train_enhanced))  # Process more images\n",
    "image_features_list = []\n",
    "\n",
    "print(f\"\\nExtracting enhanced visual features from {sample_size} images...\")\n",
    "\n",
    "for i in range(sample_size):\n",
    "    mushroom_id = train_enhanced.iloc[i]['id']\n",
    "    image_path = f\"{train_img_dir}/champignon_{mushroom_id}.png\"\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        features = extract_enhanced_image_features(image_path)\n",
    "        if features is not None:\n",
    "            features['id'] = mushroom_id\n",
    "            features['est_toxique'] = train_enhanced.iloc[i]['est_toxique']\n",
    "            image_features_list.append(features)\n",
    "    \n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"Processed {i + 1} images...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "enhanced_image_features_df = pd.DataFrame(image_features_list)\n",
    "print(f\"\\nExtracted enhanced visual features for {len(enhanced_image_features_df)} images\")\n",
    "print(f\"Enhanced image features shape: {enhanced_image_features_df.shape}\")\n",
    "\n",
    "# Validate enhanced visual features\n",
    "visual_feature_cols = [col for col in enhanced_image_features_df.columns if col not in ['id', 'est_toxique']]\n",
    "visual_correlations = enhanced_image_features_df[visual_feature_cols + ['est_toxique']].corr()['est_toxique'].abs().sort_values(ascending=False)\n",
    "visual_correlations = visual_correlations.drop('est_toxique')\n",
    "\n",
    "print(f\"\\n=== TOP ENHANCED VISUAL FEATURE CORRELATIONS ===\")\n",
    "print(visual_correlations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Multi-Modal Architecture with Feature Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Multi-Modal Model with Feature Importance Weighting\n",
    "class EnhancedMushroomNet(nn.Module):\n",
    "    def __init__(self, tabular_input_size, num_classes=2, dropout_rate=0.4):\n",
    "        super(EnhancedMushroomNet, self).__init__()\n",
    "        \n",
    "        # Enhanced Vision backbone with EfficientNet (better for feature extraction)\n",
    "        from torchvision.models import efficientnet_b0\n",
    "        try:\n",
    "            self.vision_backbone = efficientnet_b0(pretrained=True)\n",
    "            vision_feature_size = self.vision_backbone.classifier[1].in_features\n",
    "            self.vision_backbone.classifier = nn.Identity()  # Remove classifier\n",
    "        except:\n",
    "            # Fallback to ResNet50 if EfficientNet not available\n",
    "            self.vision_backbone = models.resnet50(pretrained=True)\n",
    "            self.vision_backbone = nn.Sequential(*list(self.vision_backbone.children())[:-1])\n",
    "            vision_feature_size = 2048\n",
    "        \n",
    "        # Feature importance weights based on your analysis\n",
    "        self.feature_weights = nn.Parameter(torch.ones(tabular_input_size))\n",
    "        \n",
    "        # Enhanced vision processor with more capacity\n",
    "        self.vision_processor = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)) if vision_feature_size == 2048 else nn.Identity(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(vision_feature_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        # Enhanced tabular processor with feature weighting\n",
    "        self.tabular_processor = nn.Sequential(\n",
    "            nn.Linear(tabular_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        fusion_input_size = 256 + 64  # Vision + Tabular features\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=fusion_input_size, num_heads=8, dropout=dropout_rate, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Enhanced fusion with residual connections\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Feature importance prediction head\n",
    "        self.importance_head = nn.Sequential(\n",
    "            nn.Linear(fusion_input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, tabular_input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, tabular_input, image_input):\n",
    "        # Apply learned feature weights to tabular input\n",
    "        weighted_tabular = tabular_input * torch.softmax(self.feature_weights, dim=0)\n",
    "        \n",
    "        # Process vision input\n",
    "        vision_features = self.vision_backbone(image_input)\n",
    "        if len(vision_features.shape) > 2:\n",
    "            vision_features = torch.flatten(vision_features, 1)\n",
    "        vision_output = self.vision_processor(vision_features)\n",
    "        \n",
    "        # Process tabular input\n",
    "        tabular_output = self.tabular_processor(weighted_tabular)\n",
    "        \n",
    "        # Concatenate features\n",
    "        fused_features = torch.cat([vision_output, tabular_output], dim=1)\n",
    "        \n",
    "        # Apply cross-attention\n",
    "        fused_expanded = fused_features.unsqueeze(1)  # Add sequence dimension\n",
    "        attended_features, attention_weights = self.cross_attention(\n",
    "            fused_expanded, fused_expanded, fused_expanded\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1)  # Remove sequence dimension\n",
    "        \n",
    "        # Residual connection\n",
    "        fused_features = fused_features + attended_features\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fusion(fused_features)\n",
    "        \n",
    "        # Feature importance scores\n",
    "        importance_scores = self.importance_head(fused_features)\n",
    "        \n",
    "        return output, importance_scores\n",
    "\n",
    "print(\"Enhanced multi-modal model with feature weighting defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset with probability features\n",
    "class EnhancedMushroomDataset(Dataset):\n",
    "    def __init__(self, tabular_data, image_dir, transform=None, is_test=False):\n",
    "        self.tabular_data = tabular_data\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Prepare enhanced tabular features\n",
    "        self.prepare_tabular_features()\n",
    "        \n",
    "    def prepare_tabular_features(self):\n",
    "        self.feature_data = self.tabular_data.copy()\n",
    "        \n",
    "        # Enhanced categorical encoding with target encoding for high-cardinality features\n",
    "        categorical_cols = ['odeur', 'texture', 'type_sol', 'presence_insecte', \n",
    "                           'effet_au_toucher', 'a_l_air_delicieux_selon_renard']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in self.feature_data.columns:\n",
    "                le = LabelEncoder()\n",
    "                self.feature_data[col] = le.fit_transform(self.feature_data[col])\n",
    "        \n",
    "        # Select enhanced features (including new probability features)\n",
    "        feature_cols = [col for col in self.feature_data.columns \n",
    "                       if col not in ['id', 'est_toxique']]\n",
    "        \n",
    "        self.features = self.feature_data[feature_cols].values\n",
    "        \n",
    "        # Enhanced preprocessing with power transformation\n",
    "        self.scaler = StandardScaler()\n",
    "        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "        \n",
    "        # Apply power transformation to reduce skewness\n",
    "        features_transformed = self.power_transformer.fit_transform(self.features)\n",
    "        # Then standardize\n",
    "        self.features = self.scaler.fit_transform(features_transformed)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.labels = self.feature_data['est_toxique'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tabular_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get tabular features\n",
    "        tabular_features = torch.FloatTensor(self.features[idx])\n",
    "        \n",
    "        # Get image\n",
    "        mushroom_id = self.tabular_data.iloc[idx]['id']\n",
    "        image_path = f\"{self.image_dir}/champignon_{mushroom_id}.png\"\n",
    "        \n",
    "        # Load image with better error handling\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except:\n",
    "                image = Image.new('RGB', (224, 224), color='gray')\n",
    "        else:\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return tabular_features, image, mushroom_id\n",
    "        else:\n",
    "            label = torch.LongTensor([self.labels[idx]])\n",
    "            return tabular_features, image, label\n",
    "\n",
    "print(\"Enhanced dataset class with probability features defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Training with Focus on Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data preparation\n",
    "train_split, val_split = train_test_split(\n",
    "    train_enhanced, test_size=0.2, random_state=42, \n",
    "    stratify=train_enhanced['est_toxique']\n",
    ")\n",
    "\n",
    "print(f\"Training split: {len(train_split)} samples\")\n",
    "print(f\"Validation split: {len(val_split)} samples\")\n",
    "\n",
    "# Enhanced transforms with more aggressive augmentation\n",
    "enhanced_train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.3),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "enhanced_val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create enhanced datasets\n",
    "enhanced_train_dataset = EnhancedMushroomDataset(train_split, train_img_dir, enhanced_train_transform)\n",
    "enhanced_val_dataset = EnhancedMushroomDataset(val_split, train_img_dir, enhanced_val_transform)\n",
    "\n",
    "# Weighted sampling to handle class imbalance\n",
    "class_counts = np.bincount(train_split['est_toxique'])\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[label] for label in train_split['est_toxique']]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "# Enhanced data loaders\n",
    "batch_size = 16  # Smaller batch size for better gradient updates\n",
    "enhanced_train_loader = DataLoader(enhanced_train_dataset, batch_size=batch_size, \n",
    "                                  sampler=sampler, num_workers=0)\n",
    "enhanced_val_loader = DataLoader(enhanced_val_dataset, batch_size=batch_size, \n",
    "                                shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Enhanced training batches: {len(enhanced_train_loader)}\")\n",
    "print(f\"Enhanced validation batches: {len(enhanced_val_loader)}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize enhanced model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get input size for enhanced tabular features\n",
    "enhanced_tabular_input_size = len([col for col in train_enhanced.columns \n",
    "                                  if col not in ['id', 'est_toxique']])\n",
    "print(f\"Enhanced tabular input size: {enhanced_tabular_input_size}\")\n",
    "\n",
    "enhanced_model = EnhancedMushroomNet(tabular_input_size=enhanced_tabular_input_size)\n",
    "enhanced_model = enhanced_model.to(device)\n",
    "\n",
    "# Enhanced loss function with class weights\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Enhanced optimizer with different learning rates for different components\n",
    "vision_params = list(enhanced_model.vision_backbone.parameters()) + \\\n",
    "               list(enhanced_model.vision_processor.parameters())\n",
    "tabular_params = list(enhanced_model.tabular_processor.parameters()) + \\\n",
    "                list(enhanced_model.fusion.parameters())\n",
    "attention_params = list(enhanced_model.cross_attention.parameters())\n",
    "importance_params = list(enhanced_model.importance_head.parameters()) + \\\n",
    "                  [enhanced_model.feature_weights]\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': vision_params, 'lr': 0.0001},      # Lower LR for pretrained vision\n",
    "    {'params': tabular_params, 'lr': 0.001},      # Higher LR for tabular\n",
    "    {'params': attention_params, 'lr': 0.0005},   # Medium LR for attention\n",
    "    {'params': importance_params, 'lr': 0.001}    # Higher LR for importance\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Enhanced scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "print(\"Enhanced model, loss function, and optimizer initialized!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in enhanced_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training functions with feature importance loss\n",
    "def enhanced_train_epoch(model, train_loader, criterion, optimizer, device, lambda_importance=0.1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_importance_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (tabular_data, images, labels) in enumerate(train_loader):\n",
    "        tabular_data = tabular_data.to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with importance scores\n",
    "        outputs, importance_scores = model(tabular_data, images)\n",
    "        \n",
    "        # Main classification loss\n",
    "        main_loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Feature importance regularization loss\n",
    "        # Encourage sparsity in feature importance\n",
    "        importance_loss = torch.mean(torch.abs(importance_scores))\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = main_loss + lambda_importance * importance_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += main_loss.item()\n",
    "        running_importance_loss += importance_loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Main Loss: {main_loss.item():.4f}, '\n",
    "                  f'Importance Loss: {importance_loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_importance_loss = running_importance_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_importance_loss, epoch_acc\n",
    "\n",
    "def enhanced_validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_importance_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tabular_data, images, labels in val_loader:\n",
    "            tabular_data = tabular_data.to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            \n",
    "            outputs, importance_scores = model(tabular_data, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # Toxic probabilities\n",
    "            all_importance_scores.extend(importance_scores.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_predictions, all_labels, all_probabilities, all_importance_scores\n",
    "\n",
    "print(\"Enhanced training and validation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training loop with early stopping and model checkpointing\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "best_val_auc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "train_importance_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_aucs = []\n",
    "\n",
    "print(f\"Starting enhanced training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_importance_loss, train_acc = enhanced_train_epoch(\n",
    "        enhanced_model, enhanced_train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_predictions, val_labels, val_probabilities, val_importance_scores = \\\n",
    "        enhanced_validate_epoch(enhanced_model, enhanced_val_loader, criterion, device)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    val_auc = roc_auc_score(val_labels, val_probabilities)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_importance_losses.append(train_importance_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    val_aucs.append(val_auc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val AUC: {val_auc:.4f}\")\n",
    "    print(f\"Importance Loss: {train_importance_loss:.4f}\")\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model based on AUC (better for imbalanced data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(enhanced_model.state_dict(), 'best_enhanced_mushroom_model.pth')\n",
    "        patience_counter = 0\n",
    "        print(f\"New best validation AUC: {best_val_auc:.4f} (Acc: {best_val_acc:.2f}%)\")\n",
    "        \n",
    "        # Save feature importance scores\n",
    "        avg_importance = np.mean(val_importance_scores, axis=0)\n",
    "        np.save('feature_importance_scores.npy', avg_importance)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and perform comprehensive evaluation\n",
    "enhanced_model.load_state_dict(torch.load('best_enhanced_mushroom_model.pth'))\n",
    "enhanced_model.eval()\n",
    "\n",
    "# Get final validation predictions\n",
    "final_val_loss, final_val_acc, final_predictions, final_labels, final_probabilities, final_importance_scores = \\\n",
    "    enhanced_validate_epoch(enhanced_model, enhanced_val_loader, criterion, device)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(final_labels, final_predictions)\n",
    "precision = precision_score(final_labels, final_predictions)\n",
    "recall = recall_score(final_labels, final_predictions)\n",
    "f1 = f1_score(final_labels, final_predictions)\n",
    "auc = roc_auc_score(final_labels, final_probabilities)\n",
    "\n",
    "print(\"=== ENHANCED MODEL PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(final_labels, final_predictions, \n",
    "                          target_names=['Non-toxic', 'Toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization of results\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Training history\n",
    "plt.subplot(3, 4, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 4, 2)\n",
    "plt.plot(train_accs, label='Training Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.title('Accuracy History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 4, 3)\n",
    "plt.plot(val_aucs, label='Validation AUC', color='green')\n",
    "plt.title('AUC History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 4, 4)\n",
    "plt.plot(train_importance_losses, label='Importance Loss', color='orange')\n",
    "plt.title('Feature Importance Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Importance Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(3, 4, 5)\n",
    "cm = confusion_matrix(final_labels, final_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-toxic', 'Toxic'], \n",
    "            yticklabels=['Non-toxic', 'Toxic'])\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(3, 4, 6)\n",
    "fpr, tpr, thresholds = roc_curve(final_labels, final_probabilities)\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Probability distribution\n",
    "plt.subplot(3, 4, 7)\n",
    "toxic_probs = [p for p, l in zip(final_probabilities, final_labels) if l == 1]\n",
    "non_toxic_probs = [p for p, l in zip(final_probabilities, final_labels) if l == 0]\n",
    "plt.hist(non_toxic_probs, alpha=0.7, label='Non-toxic', bins=30, density=True)\n",
    "plt.hist(toxic_probs, alpha=0.7, label='Toxic', bins=30, density=True)\n",
    "plt.xlabel('Predicted Probability of Toxicity')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Probability Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Feature importance heatmap\n",
    "plt.subplot(3, 4, 8)\n",
    "avg_importance = np.mean(final_importance_scores, axis=0)\n",
    "feature_names = [col for col in train_enhanced.columns if col not in ['id', 'est_toxique']]\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names[:len(avg_importance)],\n",
    "    'importance': avg_importance\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "plt.yticks(range(len(importance_df)), importance_df['feature'], fontsize=8)\n",
    "plt.xlabel('Learned Feature Importance')\n",
    "plt.title('Model Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Enhanced features validation\n",
    "plt.subplot(3, 4, 9)\n",
    "val_data_with_probs = val_split.copy()\n",
    "val_data_with_probs['predicted_prob'] = final_probabilities\n",
    "\n",
    "plt.scatter(val_data_with_probs['probability_of_toxicity'], \n",
    "           val_data_with_probs['predicted_prob'], alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('Engineered Toxicity Probability')\n",
    "plt.ylabel('Model Predicted Probability')\n",
    "plt.title('Feature Engineering Validation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Location-based predictions\n",
    "plt.subplot(3, 4, 10)\n",
    "scatter = plt.scatter(val_data_with_probs['x'], val_data_with_probs['y'], \n",
    "                     c=val_data_with_probs['predicted_prob'], \n",
    "                     alpha=0.6, cmap='RdYlBu_r')\n",
    "plt.colorbar(scatter, label='Predicted Toxicity Probability')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.title('Spatial Distribution of Predictions')\n",
    "\n",
    "# pH analysis\n",
    "plt.subplot(3, 4, 11)\n",
    "plt.scatter(val_data_with_probs['ph_du_jus'], val_data_with_probs['ph_du_sol'], \n",
    "           c=val_data_with_probs['predicted_prob'], alpha=0.6, cmap='RdYlBu_r')\n",
    "plt.colorbar(label='Predicted Toxicity Probability')\n",
    "plt.xlabel('pH du jus')\n",
    "plt.ylabel('pH du sol')\n",
    "plt.title('pH vs Predicted Toxicity')\n",
    "\n",
    "# Model summary\n",
    "plt.subplot(3, 4, 12)\n",
    "plt.text(0.1, 0.9, f\"Best Validation AUC: {best_val_auc:.4f}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.8, f\"Best Validation Accuracy: {best_val_acc:.2f}%\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.7, f\"Final F1-Score: {f1:.4f}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.6, f\"Enhanced Features: {len(feature_names)}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.5, f\"Total Parameters: {sum(p.numel() for p in enhanced_model.parameters()):,}\", fontsize=10, transform=plt.gca().transAxes)\n",
    "plt.axis('off')\n",
    "plt.title('Enhanced Model Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Predictions with Probability of Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced test dataset\n",
    "enhanced_test_dataset = EnhancedMushroomDataset(test_enhanced, test_img_dir, enhanced_val_transform, is_test=True)\n",
    "enhanced_test_loader = DataLoader(enhanced_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Enhanced test set size: {len(enhanced_test_dataset)}\")\n",
    "print(f\"Enhanced test batches: {len(enhanced_test_loader)}\")\n",
    "\n",
    "# Generate enhanced test predictions\n",
    "enhanced_model.eval()\n",
    "test_predictions = []\n",
    "test_importance_scores = []\n",
    "test_ids = []\n",
    "\n",
    "print(\"\\nGenerating enhanced test predictions...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (tabular_data, images, ids) in enumerate(enhanced_test_loader):\n",
    "        tabular_data = tabular_data.to(device)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        outputs, importance_scores = enhanced_model(tabular_data, images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get probability of being toxic (class 1)\n",
    "        toxic_probs = probabilities[:, 1].cpu().numpy()\n",
    "        \n",
    "        test_predictions.extend(toxic_probs)\n",
    "        test_importance_scores.extend(importance_scores.cpu().numpy())\n",
    "        test_ids.extend(ids.numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {batch_idx + 1}/{len(enhanced_test_loader)} batches\")\n",
    "\n",
    "print(f\"\\nGenerated enhanced predictions for {len(test_predictions)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive submission with additional analysis\n",
    "enhanced_submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'probabilite_toxique': test_predictions\n",
    "})\n",
    "\n",
    "# Add engineered probability for comparison\n",
    "test_enhanced_subset = test_enhanced[test_enhanced['id'].isin(test_ids)].copy()\n",
    "test_enhanced_subset = test_enhanced_subset.sort_values('id')\n",
    "enhanced_submission = enhanced_submission.sort_values('id')\n",
    "\n",
    "# Add engineered features to submission for analysis\n",
    "enhanced_submission['engineered_toxicity_prob'] = test_enhanced_subset['probability_of_toxicity'].values\n",
    "enhanced_submission['location_toxicity_prob'] = test_enhanced_subset['location_toxicity_prob'].values\n",
    "enhanced_submission['ph_toxicity_score'] = test_enhanced_subset['ph_toxicity_score'].values\n",
    "\n",
    "# Save main submission file\n",
    "final_submission = enhanced_submission[['id', 'probabilite_toxique']].copy()\n",
    "final_submission.to_csv('enhanced_y_test_predictions.csv', index=False)\n",
    "\n",
    "# Save detailed analysis file\n",
    "enhanced_submission.to_csv('detailed_test_analysis.csv', index=False)\n",
    "\n",
    "print(\"Enhanced submission files saved:\")\n",
    "print(\"  - enhanced_y_test_predictions.csv: Main submission file\")\n",
    "print(\"  - detailed_test_analysis.csv: Detailed analysis with engineered features\")\n",
    "\n",
    "print(f\"\\n=== ENHANCED PREDICTION STATISTICS ===\")\n",
    "print(f\"Mean probability: {np.mean(test_predictions):.4f}\")\n",
    "print(f\"Std probability: {np.std(test_predictions):.4f}\")\n",
    "print(f\"Min probability: {np.min(test_predictions):.4f}\")\n",
    "print(f\"Max probability: {np.max(test_predictions):.4f}\")\n",
    "print(f\"Predicted toxic (>0.5): {np.sum(np.array(test_predictions) > 0.5)}\")\n",
    "print(f\"Predicted toxic (>0.3): {np.sum(np.array(test_predictions) > 0.3)}\")\n",
    "print(f\"Predicted toxic (>0.7): {np.sum(np.array(test_predictions) > 0.7)}\")\n",
    "\n",
    "# Correlation between model predictions and engineered features\n",
    "model_vs_engineered_corr = np.corrcoef(test_predictions, enhanced_submission['engineered_toxicity_prob'])[0, 1]\n",
    "print(f\"\\nCorrelation between model predictions and engineered toxicity probability: {model_vs_engineered_corr:.4f}\")\n",
    "\n",
    "print(f\"\\n=== SUBMISSION PREVIEW ===\")\n",
    "print(final_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Key Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ENHANCED MUSHROOM TOXICITY ANALYSIS SUMMARY ===\")\n",
    "print(\"\\n🎯 KEY IMPROVEMENTS IMPLEMENTED:\")\n",
    "print(\"\\n1. ENHANCED FEATURE ENGINEERING:\")\n",
    "print(\"   ✓ Location-based toxicity probability (distance from clusters)\")\n",
    "print(\"   ✓ pH toxicity zones based on your findings\")\n",
    "print(\"   ✓ Soil type toxicity mapping\")\n",
    "print(\"   ✓ Enhanced insect presence weighting (+15% boost)\")\n",
    "print(\"   ✓ Touch effect toxicity indicators (picottement emphasis)\")\n",
    "print(\"   ✓ Comprehensive 'probability_of_toxicity' feature\")\n",
    "\n",
    "print(\"\\n2. ENHANCED VISUAL ANALYSIS:\")\n",
    "print(\"   ✓ Focus on key indicators: unique_colors, brightness, red channel\")\n",
    "print(\"   ✓ Advanced texture analysis with multiple edge detection\")\n",
    "print(\"   ✓ Color dominance and entropy measures\")\n",
    "print(\"   ✓ Visual toxicity probability score\")\n",
    "\n",
    "print(\"\\n3. ADVANCED MODEL ARCHITECTURE:\")\n",
    "print(\"   ✓ Enhanced multi-modal fusion with cross-attention\")\n",
    "print(\"   ✓ Learnable feature importance weights\")\n",
    "print(\"   ✓ Class-weighted loss for imbalanced data\")\n",
    "print(\"   ✓ Multiple learning rates for different components\")\n",
    "print(\"   ✓ Feature importance regularization\")\n",
    "\n",
    "print(\"\\n4. ROBUST TRAINING ENHANCEMENTS:\")\n",
    "print(\"   ✓ Weighted sampling for class balance\")\n",
    "print(\"   ✓ Advanced data augmentation\")\n",
    "print(\"   ✓ Early stopping with AUC optimization\")\n",
    "print(\"   ✓ Gradient clipping for stability\")\n",
    "print(\"   ✓ Power transformation for feature normalization\")\n",
    "\n",
    "print(f\"\\n📊 PERFORMANCE METRICS:\")\n",
    "print(f\"   - Best Validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"   - Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   - Final F1-Score: {f1:.4f}\")\n",
    "print(f\"   - Enhanced Features: {len(feature_names)}\")\n",
    "print(f\"   - Model Parameters: {sum(p.numel() for p in enhanced_model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY FINDING IMPLEMENTATIONS:\")\n",
    "print(f\"   ✓ Non-toxic cluster (200,200) vs Toxic cluster (0,0) - Location probability\")\n",
    "print(f\"   ✓ pH_jus toxic zones (3-6, 8-11) - Implemented as binary indicators\")\n",
    "print(f\"   ✓ pH_sol toxic zones (0-4.5, 8-12) - Implemented as binary indicators\")\n",
    "print(f\"   ✓ Soil type toxicity (eau=low, terre=high) - Mapped to probabilities\")\n",
    "print(f\"   ✓ Insect presence +15% toxicity - Explicit boost feature\")\n",
    "print(f\"   ✓ Picottement high toxicity indicator - Enhanced weighting\")\n",
    "print(f\"   ✓ Visual correlation focus (unique_colors, brightness, red) - Dedicated scores\")\n",
    "\n",
    "print(f\"\\n📁 FILES GENERATED:\")\n",
    "print(f\"   - best_enhanced_mushroom_model.pth: Enhanced model weights\")\n",
    "print(f\"   - enhanced_y_test_predictions.csv: Main submission file\")\n",
    "print(f\"   - detailed_test_analysis.csv: Detailed analysis with all features\")\n",
    "print(f\"   - feature_importance_scores.npy: Learned feature importance weights\")\n",
    "\n",
    "print(f\"\\n🚀 INNOVATION HIGHLIGHTS:\")\n",
    "print(f\"   ✓ Domain knowledge integration from your analysis\")\n",
    "print(f\"   ✓ Multi-modal fusion with attention mechanism\")\n",
    "print(f\"   ✓ Learnable feature importance with regularization\")\n",
    "print(f\"   ✓ Enhanced visual toxicity indicators\")\n",
    "print(f\"   ✓ Comprehensive probability of toxicity feature\")\n",
    "print(f\"   ✓ Advanced training strategies for imbalanced data\")\n",
    "\n",
    "print(f\"\\n✅ ANALYSIS COMPLETED SUCCESSFULLY! 🍄\")\n",
    "print(f\"\\nThe enhanced model now focuses heavily on the key indicators you identified,\")\n",
    "print(f\"with a comprehensive 'probability_of_toxicity' feature that combines all findings!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
