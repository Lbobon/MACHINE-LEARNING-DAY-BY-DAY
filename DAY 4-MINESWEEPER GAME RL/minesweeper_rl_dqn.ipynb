{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minesweeper RL: Dueling Double DQN Agent with Visualizations\n",
    "\n",
    "This notebook trains a powerful Reinforcement Learning agent to play Minesweeper using a Dueling Double DQN with a convolutional network. It includes:\n",
    "\n",
    "- A clean Minesweeper environment with flood-fill reveals and optional first-click safety\n",
    "- A Dueling Double DQN agent with a target network, action masking, experience replay, and epsilon-greedy exploration\n",
    "- Training metrics: rewards, win-rate, episode length, epsilon\n",
    "- Rich visualizations: heatmaps of Q-values, animated gameplay, and board rendering\n",
    "\n",
    "You can tune board size, number of mines, and training hyperparameters at the top of the Training section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15.0\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting gym==0.26.2\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.0)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.1)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.74.0)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.1.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.1)\n",
      "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:03\u001b[0mm\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827726 sha256=b07120cee978f63ee9b2c2f7dd261884b0e9f6b0a5e9a65db9d0854d29eaa8f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, ml-dtypes, gym, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.17.2\n",
      "    Uninstalling wrapt-1.17.2:\n",
      "      Successfully uninstalled wrapt-1.17.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.10.0\n",
      "    Uninstalling keras-3.10.0:\n",
      "      Successfully uninstalled keras-3.10.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.3\n",
      "    Uninstalling ml_dtypes-0.5.3:\n",
      "      Successfully uninstalled ml_dtypes-0.5.3\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.25.2\n",
      "    Uninstalling gym-0.25.2:\n",
      "      Successfully uninstalled gym-0.25.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.19.0\n",
      "    Uninstalling tensorboard-2.19.0:\n",
      "      Successfully uninstalled tensorboard-2.19.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.19.0\n",
      "    Uninstalling tensorflow-2.19.0:\n",
      "      Successfully uninstalled tensorflow-2.19.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorstore 0.1.76 requires ml_dtypes>=0.5.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.15.0 which is incompatible.\n",
      "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.15.0 which is incompatible.\n",
      "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.15.0 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.5.3 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "keras-hub 0.21.1 requires keras>=3.5, but you have keras 2.15.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gym-0.26.2 keras-2.15.0 ml-dtypes-0.2.0 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "1ade10eebedd418a9112e81ce22cff93",
       "pip_warning": {
        "packages": [
         "google",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pip install tensorflow==2.15.0 gym==0.26.2 seaborn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animation\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models, optimizers, losses\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Reproducibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plot styling\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Minesweeper Environment with Better Rewards\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "class MinesweeperEnv:\n",
    "    \"\"\"\n",
    "    Enhanced Minesweeper environment with improved reward structure:\n",
    "      - Progressive revealing rewards that encourage strategic play\n",
    "      - Heavy penalties for poor actions\n",
    "      - Dense reward shaping for better learning\n",
    "    \"\"\"\n",
    "    def __init__(self, height=9, width=9, num_mines=10, first_click_safe=True, seed=None):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_mines = num_mines\n",
    "        self.first_click_safe = first_click_safe\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.height * self.width)\n",
    "        # Enhanced observation with safety indicators\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(self.height, self.width, 13), dtype=np.float32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.mines = np.zeros((self.height, self.width), dtype=bool)\n",
    "        self.revealed = np.zeros((self.height, self.width), dtype=bool)\n",
    "        self.flagged = np.zeros((self.height, self.width), dtype=bool)\n",
    "        self.neighbor_counts = np.zeros((self.height, self.width), dtype=np.int8)\n",
    "        self.num_safe_cells = self.height * self.width - self.num_mines\n",
    "        self.revealed_safe = 0\n",
    "        self.done = False\n",
    "        self.first_action_done = False\n",
    "        self.total_reward = 0.0\n",
    "        self.steps = 0\n",
    "        # Track consecutive safe reveals for bonus\n",
    "        self.consecutive_safe = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _place_mines(self, safe_r=None, safe_c=None):\n",
    "        positions = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        if self.first_click_safe and safe_r is not None:\n",
    "            # More generous safe zone around first click\n",
    "            excl = set()\n",
    "            for rr in range(max(0, safe_r-2), min(self.height, safe_r+3)):\n",
    "                for cc in range(max(0, safe_c-2), min(self.width, safe_c+3)):\n",
    "                    excl.add((rr, cc))\n",
    "            positions = [pos for pos in positions if pos not in excl]\n",
    "        self.rng.shuffle(positions)\n",
    "        for (r, c) in positions[: self.num_mines]:\n",
    "            self.mines[r, c] = True\n",
    "        # Neighbor counts\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                if self.mines[r, c]:\n",
    "                    self.neighbor_counts[r, c] = -1\n",
    "                else:\n",
    "                    self.neighbor_counts[r, c] = self._count_neighbors(r, c)\n",
    "\n",
    "    def _count_neighbors(self, r, c):\n",
    "        count = 0\n",
    "        for rr in range(max(0, r-1), min(self.height, r+2)):\n",
    "            for cc in range(max(0, c-1), min(self.width, c+2)):\n",
    "                if rr == r and cc == c:\n",
    "                    continue\n",
    "                if self.mines[rr, cc]:\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "    def _flood_fill(self, r, c):\n",
    "        stack = [(r, c)]\n",
    "        newly_revealed = []\n",
    "        while stack:\n",
    "            rr, cc = stack.pop()\n",
    "            if self.revealed[rr, cc]:\n",
    "                continue\n",
    "            self.revealed[rr, cc] = True\n",
    "            newly_revealed.append((rr, cc))\n",
    "            if self.neighbor_counts[rr, cc] == 0:\n",
    "                for nr in range(max(0, rr-1), min(self.height, rr+2)):\n",
    "                    for nc in range(max(0, cc-1), min(self.width, cc+2)):\n",
    "                        if not self.revealed[nr, nc] and not self.mines[nr, nc]:\n",
    "                            stack.append((nr, nc))\n",
    "        return newly_revealed\n",
    "\n",
    "    def _calculate_progress_reward(self, newly_revealed_count):\n",
    "        \"\"\"Enhanced progress-based reward\"\"\"\n",
    "        base_reward = newly_revealed_count * 2.0  # Higher base reward\n",
    "        \n",
    "        # Bonus for consecutive safe reveals\n",
    "        self.consecutive_safe += newly_revealed_count\n",
    "        consecutive_bonus = min(self.consecutive_safe * 0.5, 10.0)\n",
    "        \n",
    "        # Progress bonus - more reward as we get closer to winning\n",
    "        progress = self.revealed_safe / self.num_safe_cells\n",
    "        progress_bonus = progress * 15.0  # Increasing reward as we approach win\n",
    "        \n",
    "        # Efficiency bonus - reward fewer steps\n",
    "        efficiency_bonus = max(0, (self.num_safe_cells - self.steps) * 0.1)\n",
    "        \n",
    "        return base_reward + consecutive_bonus + progress_bonus + efficiency_bonus\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0.0, True, {}\n",
    "\n",
    "        self.steps += 1\n",
    "        r, c = divmod(int(action), self.width)\n",
    "\n",
    "        # Lazy mine placement on first action\n",
    "        if not self.first_action_done:\n",
    "            self._place_mines(safe_r=r, safe_c=c)\n",
    "            self.first_action_done = True\n",
    "\n",
    "        reward = 0.0\n",
    "        info = {}\n",
    "\n",
    "        # Heavy penalty for clicking already revealed cells\n",
    "        if self.revealed[r, c]:\n",
    "            reward = -5.0  # Much higher penalty\n",
    "            self.consecutive_safe = 0  # Reset streak\n",
    "            return self._get_obs(), reward, False, info\n",
    "\n",
    "        # Mine hit - game over with severe penalty\n",
    "        if self.mines[r, c]:\n",
    "            self.revealed[r, c] = True\n",
    "            self.done = True\n",
    "            # Penalty based on how much progress was made\n",
    "            progress_penalty = -(50.0 - self.revealed_safe * 2.0)\n",
    "            reward = progress_penalty\n",
    "            self.consecutive_safe = 0\n",
    "            return self._get_obs(), reward, True, info\n",
    "\n",
    "        # Safe cell reveal\n",
    "        before = np.sum(self.revealed)\n",
    "        newly = self._flood_fill(r, c)\n",
    "        after = np.sum(self.revealed)\n",
    "        gained = after - before\n",
    "        self.revealed_safe += gained\n",
    "\n",
    "        # Calculate sophisticated reward\n",
    "        reward = self._calculate_progress_reward(gained)\n",
    "        \n",
    "        # Small step penalty to encourage efficiency\n",
    "        reward -= 0.05\n",
    "\n",
    "        # Win condition with massive bonus\n",
    "        if self.revealed_safe >= self.num_safe_cells:\n",
    "            self.done = True\n",
    "            # Win bonus scales with efficiency\n",
    "            win_bonus = 100.0 + max(0, (self.num_safe_cells - self.steps) * 2.0)\n",
    "            reward += win_bonus\n",
    "\n",
    "        self.total_reward += reward\n",
    "        return self._get_obs(), reward, self.done, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        H, W = self.height, self.width\n",
    "        # Enhanced observation with safety and mine probability indicators\n",
    "        obs = np.zeros((H, W, 13), dtype=np.float32)\n",
    "        obs[..., 0] = self.revealed.astype(np.float32)\n",
    "        obs[..., 1] = self.flagged.astype(np.float32)\n",
    "        \n",
    "        # One-hot encoding for neighbor counts (0-8)\n",
    "        for r in range(H):\n",
    "            for c in range(W):\n",
    "                if self.revealed[r, c]:\n",
    "                    count = int(self.neighbor_counts[r, c])\n",
    "                    if count < 0:\n",
    "                        count = 0\n",
    "                    obs[r, c, 2 + count] = 1.0\n",
    "        \n",
    "        # Safety indicator channel - helps agent identify safer moves\n",
    "        obs[..., 11] = self._compute_safety_map()\n",
    "        \n",
    "        # Progress indicator - normalized progress towards win\n",
    "        obs[..., 12] = self.revealed_safe / self.num_safe_cells\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _compute_safety_map(self):\n",
    "        \"\"\"Compute relative safety of unrevealed cells based on revealed neighbors\"\"\"\n",
    "        safety = np.zeros((self.height, self.width), dtype=np.float32)\n",
    "        \n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                if not self.revealed[r, c]:\n",
    "                    # Count revealed neighbors and their mine counts\n",
    "                    revealed_neighbors = 0\n",
    "                    total_neighbor_mines = 0\n",
    "                    \n",
    "                    for rr in range(max(0, r-1), min(self.height, r+2)):\n",
    "                        for cc in range(max(0, c-1), min(self.width, c+2)):\n",
    "                            if rr == r and cc == c:\n",
    "                                continue\n",
    "                            if self.revealed[rr, cc]:\n",
    "                                revealed_neighbors += 1\n",
    "                                if self.neighbor_counts[rr, cc] >= 0:\n",
    "                                    total_neighbor_mines += self.neighbor_counts[rr, cc]\n",
    "                    \n",
    "                    # Higher safety for cells near revealed cells with low mine counts\n",
    "                    if revealed_neighbors > 0:\n",
    "                        avg_neighbor_mines = total_neighbor_mines / revealed_neighbors\n",
    "                        safety[r, c] = max(0, 1.0 - avg_neighbor_mines / 8.0)\n",
    "                    else:\n",
    "                        safety[r, c] = 0.5  # Neutral for unexplored areas\n",
    "        \n",
    "        return safety\n",
    "\n",
    "    def render_ascii(self, reveal_mines_when_done=True):\n",
    "        H, W = self.height, self.width\n",
    "        board = []\n",
    "        for r in range(H):\n",
    "            row = []\n",
    "            for c in range(W):\n",
    "                if self.revealed[r, c]:\n",
    "                    if self.mines[r, c]:\n",
    "                        row.append('*')\n",
    "                    else:\n",
    "                        cnt = self.neighbor_counts[r, c]\n",
    "                        row.append(str(cnt))\n",
    "                else:\n",
    "                    if reveal_mines_when_done and self.done and self.mines[r, c]:\n",
    "                        row.append('M')\n",
    "                    else:\n",
    "                        row.append('□')\n",
    "            board.append(' '.join(row))\n",
    "        print('\\n'.join(board))\n",
    "\n",
    "    def valid_action_mask(self):\n",
    "        # Mask invalid actions (already revealed cells)\n",
    "        mask = (~self.revealed).astype(np.float32)\n",
    "        return mask.reshape(-1)\n",
    "\n",
    "    def sample_random_action(self):\n",
    "        # Sample only unrevealed cells\n",
    "        unrevealed = np.argwhere(~self.revealed)\n",
    "        if len(unrevealed) == 0:\n",
    "            return 0\n",
    "        idx = self.rng.randint(len(unrevealed))\n",
    "        r, c = unrevealed[idx]\n",
    "        return r * self.width + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: plotting and rendering\n",
    "\n",
    "def plot_training_curves(history):\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\tmetrics = [\n",
    "\t\t(\"episode_reward\", \"Total Reward\"),\n",
    "\t\t(\"win_rate\", \"Win Rate (moving avg)\"),\n",
    "\t\t(\"episode_length\", \"Episode Length\"),\n",
    "\t\t(\"epsilon\", \"Epsilon\")\n",
    "\t]\n",
    "\tfor ax, (k, title) in zip(axes.ravel(), metrics):\n",
    "\t\tax.plot(history[k])\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(\"Episode\")\n",
    "\t\tax.grid(True)\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def render_board_image(env: MinesweeperEnv, show_mines_when_done=True):\n",
    "\tH, W = env.height, env.width\n",
    "\timg = np.zeros((H, W, 3), dtype=np.float32)\n",
    "\tfor r in range(H):\n",
    "\t\tfor c in range(W):\n",
    "\t\t\tif env.revealed[r, c]:\n",
    "\t\t\t\tcnt = env.neighbor_counts[r, c]\n",
    "\t\t\t\tcolor = (0.85, 0.95, 1.0)\n",
    "\t\t\t\timg[r, c] = color\n",
    "\t\t\telse:\n",
    "\t\t\t\timg[r, c] = (0.6, 0.7, 0.8)\n",
    "\t\t\t\tif env.done and show_mines_when_done and env.mines[r, c]:\n",
    "\t\t\t\t\timg[r, c] = (0.9, 0.2, 0.2)\n",
    "\treturn img\n",
    "\n",
    "\n",
    "def show_board(env):\n",
    "\timg = render_board_image(env)\n",
    "\tplt.figure(figsize=(5, 5))\n",
    "\tplt.imshow(img, interpolation=\"nearest\")\n",
    "\tplt.xticks(range(env.width))\n",
    "\tplt.yticks(range(env.height))\n",
    "\tplt.grid(color='k', linestyle=':', linewidth=0.5)\n",
    "\tplt.title(\"Minesweeper Board\")\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def heatmap_q_values(q_values, env: MinesweeperEnv, title=\"Q-Values Heatmap\"):\n",
    "\tH, W = env.height, env.width\n",
    "\tgrid = q_values.reshape(H, W)\n",
    "\tplt.figure(figsize=(5, 5))\n",
    "\tsns.heatmap(grid, annot=False, cmap=\"viridis\")\n",
    "\tplt.title(title)\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling Double DQN Agent\n",
    "\n",
    "@dataclass\n",
    "class DQNConfig:\n",
    "\tlearning_rate: float = 1e-3\n",
    "\tgamma: float = 0.99\n",
    "\ttarget_update_every: int = 100\n",
    "\tbatch_size: int = 64\n",
    "\treplay_capacity: int = 100_000\n",
    "\tepsilon_start: float = 1.0\n",
    "\tepsilon_end: float = 0.05\n",
    "\tepsilon_decay_episodes: int = 800\n",
    "\tconv_channels: int = 32\n",
    "\tfc_units: int = 128\n",
    "\n",
    "\n",
    "def build_dueling_cnn(input_shape, num_actions, conv_channels=32, fc_units=128):\n",
    "\tinputs = layers.Input(shape=input_shape)\n",
    "\t# Feature extractor\n",
    "\tx = layers.Conv2D(conv_channels, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "\tx = layers.Conv2D(conv_channels, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = layers.Conv2D(conv_channels, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
    "\tx = layers.Flatten()(x)\n",
    "\t# Dueling heads\n",
    "\tvalue = layers.Dense(fc_units, activation=\"relu\")(x)\n",
    "\tvalue = layers.Dense(1)(value)\n",
    "\tadv = layers.Dense(fc_units, activation=\"relu\")(x)\n",
    "\tadv = layers.Dense(num_actions)(adv)\n",
    "\t# Q = V + (A - mean(A))\n",
    "\tadv_mean = layers.Lambda(lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(adv)\n",
    "\tq_values = layers.Add()([value, layers.Subtract()([adv, adv_mean])])\n",
    "\tmodel = models.Model(inputs=inputs, outputs=q_values)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\tdef __init__(self, capacity, obs_shape):\n",
    "\t\tself.capacity = capacity\n",
    "\t\tself.obs_shape = obs_shape\n",
    "\t\tself.states = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "\t\tself.next_states = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "\t\tself.actions = np.zeros((capacity,), dtype=np.int32)\n",
    "\t\tself.rewards = np.zeros((capacity,), dtype=np.float32)\n",
    "\t\tself.dones = np.zeros((capacity,), dtype=np.float32)\n",
    "\t\tself.index = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\tdef add(self, s, a, r, ns, d):\n",
    "\t\tidx = self.index % self.capacity\n",
    "\t\tself.states[idx] = s\n",
    "\t\tself.next_states[idx] = ns\n",
    "\t\tself.actions[idx] = a\n",
    "\t\tself.rewards[idx] = r\n",
    "\t\tself.dones[idx] = float(d)\n",
    "\t\tself.index += 1\n",
    "\t\tself.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tidxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\t\treturn (\n",
    "\t\t\tself.states[idxs],\n",
    "\t\t\tself.actions[idxs],\n",
    "\t\t\tself.rewards[idxs],\n",
    "\t\t\tself.next_states[idxs],\n",
    "\t\t\tself.dones[idxs],\n",
    "\t\t)\n",
    "\n",
    "\n",
    "class DuelingDoubleDQN:\n",
    "\tdef __init__(self, obs_shape, num_actions, config: DQNConfig):\n",
    "\t\tself.num_actions = num_actions\n",
    "\t\tself.config = config\n",
    "\t\tself.online = build_dueling_cnn(obs_shape, num_actions, config.conv_channels, config.fc_units)\n",
    "\t\tself.target = build_dueling_cnn(obs_shape, num_actions, config.conv_channels, config.fc_units)\n",
    "\t\tself.target.set_weights(self.online.get_weights())\n",
    "\t\tself.optimizer = optimizers.Adam(learning_rate=config.learning_rate)\n",
    "\t\tself.loss_fn = losses.Huber()\n",
    "\n",
    "\t\tself.replay = ReplayBuffer(config.replay_capacity, obs_shape)\n",
    "\t\tself.train_step_count = 0\n",
    "\n",
    "\tdef select_action(self, state, valid_mask, epsilon):\n",
    "\t\tif np.random.rand() < epsilon:\n",
    "\t\t\t# masked random\n",
    "\t\t\tprobs = valid_mask / (valid_mask.sum() + 1e-8)\n",
    "\t\t\treturn np.random.choice(self.num_actions, p=probs)\n",
    "\t\tq = self.online.predict(state[None, ...], verbose=0)[0]\n",
    "\t\t# mask invalid actions by -inf\n",
    "\t\tmasked_q = np.where(valid_mask > 0.5, q, -1e9)\n",
    "\t\treturn int(np.argmax(masked_q))\n",
    "\n",
    "\t@tf.function\n",
    "\tdef _train_step(self, states, actions, rewards, next_states, dones):\n",
    "\t\t# Double DQN: action selection by online, eval by target\n",
    "\t\tonline_q_next = self.online(next_states, training=False)\n",
    "\t\tnext_actions = tf.argmax(online_q_next, axis=1)\n",
    "\t\ttarget_q_next = self.target(next_states, training=False)\n",
    "\t\ttarget_q_next = tf.gather(target_q_next, next_actions, batch_dims=1)\n",
    "\t\ty = rewards + self.config.gamma * (1.0 - dones) * target_q_next\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\tq_values = self.online(states, training=True)\n",
    "\t\t\tpred = tf.gather(q_values, actions, batch_dims=1)\n",
    "\t\t\tloss = self.loss_fn(y, pred)\n",
    "\t\tgrads = tape.gradient(loss, self.online.trainable_variables)\n",
    "\t\tself.optimizer.apply_gradients(zip(grads, self.online.trainable_variables))\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef train_on_batch(self):\n",
    "\t\tif self.replay.size < self.config.batch_size:\n",
    "\t\t\treturn None\n",
    "\t\tbatch = self.replay.sample(self.config.batch_size)\n",
    "\t\tloss = self._train_step(*[tf.convert_to_tensor(x) for x in batch])\n",
    "\t\tself.train_step_count += 1\n",
    "\t\tif self.train_step_count % self.config.target_update_every == 0:\n",
    "\t\t\tself.target.set_weights(self.online.get_weights())\n",
    "\t\treturn float(loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced Dueling Double DQN Agent with Prioritized Replay\n\n@dataclass\nclass DQNConfig:\n\tlearning_rate: float = 3e-4\n\tgamma: float = 0.995\n\ttarget_update_every: int = 500\n\tbatch_size: int = 128\n\treplay_capacity: int = 200_000\n\tepsilon_start: float = 1.0\n\tepsilon_end: float = 0.02\n\tepsilon_decay_episodes: int = 1500\n\tconv_channels: int = 64\n\tfc_units: int = 256\n\t# Prioritized replay parameters\n\talpha: float = 0.6  # Priority exponent\n\tbeta_start: float = 0.4  # Importance sampling exponent\n\tbeta_end: float = 1.0\n\t# Additional improvements\n\tgradient_clip: float = 10.0\n\tweight_decay: float = 1e-5\n\n\ndef build_enhanced_dueling_cnn(input_shape, num_actions, conv_channels=64, fc_units=256):\n\t\"\"\"Enhanced CNN with residual connections and attention mechanism\"\"\"\n\tinputs = layers.Input(shape=input_shape)\n\t\n\t# Initial feature extraction with larger receptive field\n\tx = layers.Conv2D(conv_channels, kernel_size=5, padding=\"same\", activation=\"relu\")(inputs)\n\tx = layers.BatchNormalization()(x)\n\t\n\t# Residual blocks for better feature learning\n\tfor i in range(3):\n\t\tresidual = x\n\t\tx = layers.Conv2D(conv_channels, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n\t\tx = layers.BatchNormalization()(x)\n\t\tx = layers.Conv2D(conv_channels, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n\t\tx = layers.BatchNormalization()(x)\n\t\tx = layers.Add()([x, residual])  # Residual connection\n\t\tx = layers.Activation(\"relu\")(x)\n\t\n\t# Spatial attention mechanism\n\tattention = layers.Conv2D(1, kernel_size=1, activation=\"sigmoid\", name=\"spatial_attention\")(x)\n\tx = layers.Multiply()([x, attention])\n\t\n\t# Global average pooling + flatten for spatial information preservation\n\tgap = layers.GlobalAveragePooling2D()(x)\n\tflat = layers.Flatten()(x)\n\tx = layers.Concatenate()([gap, flat])\n\t\n\t# Enhanced dueling heads with dropout\n\tx = layers.Dense(fc_units, activation=\"relu\")(x)\n\tx = layers.Dropout(0.3)(x)\n\tx = layers.Dense(fc_units // 2, activation=\"relu\")(x)\n\tx = layers.Dropout(0.2)(x)\n\t\n\t# Value head\n\tvalue = layers.Dense(fc_units // 4, activation=\"relu\")(x)\n\tvalue = layers.Dense(1, name=\"value\")(value)\n\t\n\t# Advantage head\n\tadv = layers.Dense(fc_units // 2, activation=\"relu\")(x)\n\tadv = layers.Dense(num_actions, name=\"advantage\")(adv)\n\t\n\t# Dueling aggregation with learnable normalization\n\tadv_mean = layers.Lambda(lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(adv)\n\tq_values = layers.Add(name=\"q_values\")([value, layers.Subtract()([adv, adv_mean])])\n\t\n\tmodel = models.Model(inputs=inputs, outputs=q_values)\n\treturn model\n\n\nclass PrioritizedReplayBuffer:\n\t\"\"\"Fixed Prioritized Experience Replay buffer with sum tree implementation\"\"\"\n\tdef __init__(self, capacity, obs_shape, alpha=0.6):\n\t\tself.capacity = capacity\n\t\tself.obs_shape = obs_shape\n\t\tself.alpha = alpha\n\t\tself.max_priority = 1.0\n\t\t\n\t\t# Storage\n\t\tself.states = np.zeros((capacity, *obs_shape), dtype=np.float32)\n\t\tself.next_states = np.zeros((capacity, *obs_shape), dtype=np.float32)\n\t\tself.actions = np.zeros((capacity,), dtype=np.int32)\n\t\tself.rewards = np.zeros((capacity,), dtype=np.float32)\n\t\tself.dones = np.zeros((capacity,), dtype=np.float32)\n\t\t\n\t\t# Priority tree - fix size calculation\n\t\ttree_capacity = 1\n\t\twhile tree_capacity < capacity:\n\t\t\ttree_capacity *= 2\n\t\tself.tree_capacity = tree_capacity\n\t\tself.tree = np.zeros(2 * tree_capacity - 1)\n\t\t\n\t\tself.index = 0\n\t\tself.size = 0\n\n\tdef _update_tree(self, idx, priority):\n\t\ttree_idx = idx + self.tree_capacity - 1\n\t\tself.tree[tree_idx] = priority ** self.alpha\n\t\t\n\t\twhile tree_idx != 0:\n\t\t\ttree_idx = (tree_idx - 1) // 2\n\t\t\tself.tree[tree_idx] = self.tree[2 * tree_idx + 1] + self.tree[2 * tree_idx + 2]\n\n\tdef add(self, s, a, r, ns, d):\n\t\tidx = self.index % self.capacity\n\t\tself.states[idx] = s\n\t\tself.next_states[idx] = ns\n\t\tself.actions[idx] = a\n\t\tself.rewards[idx] = r\n\t\tself.dones[idx] = float(d)\n\t\t\n\t\tself._update_tree(idx, self.max_priority)\n\t\t\n\t\tself.index += 1\n\t\tself.size = min(self.size + 1, self.capacity)\n\n\tdef _sample_proportional(self, batch_size):\n\t\tindices = []\n\t\tpriorities = []\n\t\tsegment = self.tree[0] / batch_size\n\t\t\n\t\tfor i in range(batch_size):\n\t\t\ta = segment * i\n\t\t\tb = segment * (i + 1)\n\t\t\ts = np.random.uniform(a, b)\n\t\t\tidx = self._retrieve(0, s)\n\t\t\t# Ensure idx is within valid range\n\t\t\tidx = max(0, min(idx, self.tree_capacity - 1))\n\t\t\tindices.append(idx)\n\t\t\tpriorities.append(self.tree[idx + self.tree_capacity - 1])\n\t\t\n\t\treturn indices, priorities\n\n\tdef _retrieve(self, idx, s):\n\t\tleft = 2 * idx + 1\n\t\tright = left + 1\n\t\t\n\t\tif left >= len(self.tree):\n\t\t\treturn idx\n\t\t\n\t\tif s <= self.tree[left]:\n\t\t\treturn self._retrieve(left, s)\n\t\telse:\n\t\t\treturn self._retrieve(right, s - self.tree[left])\n\n\tdef sample(self, batch_size, beta=0.4):\n\t\tif self.size < batch_size:\n\t\t\t# Fallback to uniform sampling if not enough data\n\t\t\tindices = np.random.randint(0, self.size, size=batch_size)\n\t\t\tweights = np.ones(batch_size)\n\t\t\treturn (\n\t\t\t\tself.states[indices],\n\t\t\t\tself.actions[indices],\n\t\t\t\tself.rewards[indices],\n\t\t\t\tself.next_states[indices],\n\t\t\t\tself.dones[indices],\n\t\t\t\tweights,\n\t\t\t\tindices\n\t\t\t)\n\t\t\n\t\tindices, priorities = self._sample_proportional(batch_size)\n\t\t\n\t\t# Convert tree indices to buffer indices and clamp to valid range\n\t\tbuffer_indices = [min(max(0, idx), self.size - 1) for idx in indices]\n\t\t\n\t\t# Importance sampling weights\n\t\tpriorities = np.array(priorities)\n\t\tpriorities = np.maximum(priorities, 1e-8)  # Avoid division by zero\n\t\tweights = (self.size * priorities) ** (-beta)\n\t\tweights /= weights.max()\n\t\t\n\t\treturn (\n\t\t\tself.states[buffer_indices],\n\t\t\tself.actions[buffer_indices],\n\t\t\tself.rewards[buffer_indices],\n\t\t\tself.next_states[buffer_indices],\n\t\t\tself.dones[buffer_indices],\n\t\t\tweights,\n\t\t\tbuffer_indices\n\t\t)\n\n\tdef update_priorities(self, indices, priorities):\n\t\tfor idx, priority in zip(indices, priorities):\n\t\t\tif 0 <= idx < self.capacity:  # Safety check\n\t\t\t\tself.max_priority = max(self.max_priority, priority)\n\t\t\t\tself._update_tree(idx, priority)\n\n\nclass EnhancedDuelingDoubleDQN:\n\tdef __init__(self, obs_shape, num_actions, config: DQNConfig):\n\t\tself.num_actions = num_actions\n\t\tself.config = config\n\t\t\n\t\t# Build networks\n\t\tself.online = build_enhanced_dueling_cnn(obs_shape, num_actions, config.conv_channels, config.fc_units)\n\t\tself.target = build_enhanced_dueling_cnn(obs_shape, num_actions, config.conv_channels, config.fc_units)\n\t\tself.target.set_weights(self.online.get_weights())\n\t\t\n\t\t# Optimizer with weight decay\n\t\tself.optimizer = optimizers.AdamW(\n\t\t\tlearning_rate=config.learning_rate,\n\t\t\tweight_decay=config.weight_decay\n\t\t)\n\t\tself.loss_fn = losses.Huber(delta=1.0)\n\n\t\t# Prioritized replay buffer\n\t\tself.replay = PrioritizedReplayBuffer(config.replay_capacity, obs_shape, config.alpha)\n\t\tself.train_step_count = 0\n\t\t\n\t\t# For beta annealing\n\t\tself.beta_start = config.beta_start\n\t\tself.beta_end = config.beta_end\n\n\tdef select_action(self, state, valid_mask, epsilon):\n\t\tif np.random.rand() < epsilon:\n\t\t\t# Safety-weighted random action\n\t\t\tsafety_scores = state[..., 11].flatten()  # Safety channel\n\t\t\tcombined_probs = valid_mask * (1.0 + safety_scores)  # Bias towards safer cells\n\t\t\tcombined_probs = combined_probs / (combined_probs.sum() + 1e-8)\n\t\t\treturn np.random.choice(self.num_actions, p=combined_probs)\n\t\t\n\t\tq = self.online.predict(state[None, ...], verbose=0)[0]\n\t\t# Mask invalid actions and add safety bias\n\t\tsafety_scores = state[..., 11].flatten()\n\t\tsafety_bias = safety_scores * 0.1  # Small safety bias even for greedy actions\n\t\tmasked_q = np.where(valid_mask > 0.5, q + safety_bias, -1e9)\n\t\treturn int(np.argmax(masked_q))\n\n\tdef get_beta(self, episode, max_episodes):\n\t\t\"\"\"Linearly anneal beta from beta_start to beta_end\"\"\"\n\t\tfraction = min(episode / max_episodes, 1.0)\n\t\treturn self.beta_start + fraction * (self.beta_end - self.beta_start)\n\n\t@tf.function\n\tdef _train_step(self, states, actions, rewards, next_states, dones, weights):\n\t\t# Double DQN with importance sampling\n\t\tonline_q_next = self.online(next_states, training=False)\n\t\tnext_actions = tf.argmax(online_q_next, axis=1)\n\t\ttarget_q_next = self.target(next_states, training=False)\n\t\ttarget_q_next = tf.gather(target_q_next, next_actions, batch_dims=1)\n\t\ty = rewards + self.config.gamma * (1.0 - dones) * target_q_next\n\t\t\n\t\twith tf.GradientTape() as tape:\n\t\t\tq_values = self.online(states, training=True)\n\t\t\tpred = tf.gather(q_values, actions, batch_dims=1)\n\t\t\ttd_errors = y - pred\n\t\t\tloss = tf.reduce_mean(weights * self.loss_fn(y, pred))\n\t\t\n\t\t# Gradient clipping\n\t\tgrads = tape.gradient(loss, self.online.trainable_variables)\n\t\tgrads, _ = tf.clip_by_global_norm(grads, self.config.gradient_clip)\n\t\tself.optimizer.apply_gradients(zip(grads, self.online.trainable_variables))\n\t\t\n\t\treturn loss, td_errors\n\n\tdef train_on_batch(self, episode=0, max_episodes=1000):\n\t\tif self.replay.size < self.config.batch_size:\n\t\t\treturn None\n\t\t\t\n\t\tbeta = self.get_beta(episode, max_episodes)\n\t\tstates, actions, rewards, next_states, dones, weights, indices = self.replay.sample(\n\t\t\tself.config.batch_size, beta\n\t\t)\n\t\t\n\t\tloss, td_errors = self._train_step(\n\t\t\ttf.convert_to_tensor(states),\n\t\t\ttf.convert_to_tensor(actions),\n\t\t\ttf.convert_to_tensor(rewards),\n\t\t\ttf.convert_to_tensor(next_states),\n\t\t\ttf.convert_to_tensor(dones),\n\t\t\ttf.convert_to_tensor(weights, dtype=tf.float32)\n\t\t)\n\t\t\n\t\t# Update priorities\n\t\tpriorities = np.abs(td_errors.numpy()) + 1e-6\n\t\tself.replay.update_priorities(indices, priorities)\n\t\t\n\t\tself.train_step_count += 1\n\t\tif self.train_step_count % self.config.target_update_every == 0:\n\t\t\tself.target.set_weights(self.online.get_weights())\n\t\t\n\t\treturn float(loss.numpy())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training loop with curriculum learning\n",
    "\n",
    "def linear_epsilon(eps, eps_end, frac):\n",
    "\treturn max(eps_end, eps - (eps - eps_end) * frac)\n",
    "\n",
    "\n",
    "def exponential_epsilon(eps_start, eps_end, frac, decay_rate=5.0):\n",
    "\t\"\"\"Exponential epsilon decay for better exploration schedule\"\"\"\n",
    "\treturn eps_end + (eps_start - eps_end) * np.exp(-decay_rate * frac)\n",
    "\n",
    "\n",
    "def moving_average(x, k=50):\n",
    "\tif len(x) == 0:\n",
    "\t\treturn []\n",
    "\tx_arr = np.array(x, dtype=np.float32)\n",
    "\tif len(x_arr) < k:\n",
    "\t\treturn list(np.convolve(x_arr, np.ones(len(x_arr))/len(x_arr), mode='valid'))\n",
    "\treturn list(np.convolve(x_arr, np.ones(k)/k, mode='valid'))\n",
    "\n",
    "\n",
    "def train_enhanced_agent(\n",
    "\tenv: MinesweeperEnv,\n",
    "\tepisodes=2500,\n",
    "\twarmup_steps=2000,\n",
    "\tmax_steps_per_episode=None,\n",
    "\tconfig=DQNConfig(),\n",
    "\tshow_progress_every=100,\n",
    "\tcurriculum_learning=True,\n",
    "):\n",
    "\t\"\"\"Enhanced training with curriculum learning and early success detection\"\"\"\n",
    "\tobs_shape = env.observation_space.shape\n",
    "\tnum_actions = env.action_space.n\n",
    "\tagent = EnhancedDuelingDoubleDQN(obs_shape, num_actions, config)\n",
    "\n",
    "\t# History tracking\n",
    "\thistory = defaultdict(list)\n",
    "\tbest_winrate = 0.0\n",
    "\trecent_wins = deque(maxlen=50)  # Track recent performance\n",
    "\t\n",
    "\t# Curriculum learning: start with easier settings if enabled\n",
    "\tif curriculum_learning:\n",
    "\t\toriginal_mines = env.num_mines\n",
    "\t\t# Start with fewer mines for easier learning\n",
    "\t\tenv.num_mines = max(3, env.num_mines // 2)\n",
    "\t\tcurriculum_phase = 0\n",
    "\t\tphase_episodes = episodes // 3\n",
    "\t\tprint(f\"Starting curriculum with {env.num_mines} mines\")\n",
    "\n",
    "\tepsilon = config.epsilon_start\n",
    "\tconsecutive_good_episodes = 0\n",
    "\t\n",
    "\tfor ep in range(episodes):\n",
    "\t\t# Curriculum progression\n",
    "\t\tif curriculum_learning:\n",
    "\t\t\tif ep == phase_episodes and curriculum_phase == 0:\n",
    "\t\t\t\tenv.num_mines = original_mines * 3 // 4\n",
    "\t\t\t\tcurriculum_phase = 1\n",
    "\t\t\t\tprint(f\"Curriculum phase 2: {env.num_mines} mines\")\n",
    "\t\t\telif ep == 2 * phase_episodes and curriculum_phase == 1:\n",
    "\t\t\t\tenv.num_mines = original_mines\n",
    "\t\t\t\tcurriculum_phase = 2\n",
    "\t\t\t\tprint(f\"Curriculum phase 3: {env.num_mines} mines (full difficulty)\")\n",
    "\t\t\n",
    "\t\tstate = env.reset()\n",
    "\t\tdone = False\n",
    "\t\tep_reward = 0.0\n",
    "\t\tsteps = 0\n",
    "\t\tif max_steps_per_episode is None:\n",
    "\t\t\tmax_steps_per_episode = env.height * env.width * 2  # More generous step limit\n",
    "\n",
    "\t\t# Episode loop\n",
    "\t\twhile not done and steps < max_steps_per_episode:\n",
    "\t\t\tvalid_mask = env.valid_action_mask()\n",
    "\t\t\taction = agent.select_action(state, valid_mask, epsilon)\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action)\n",
    "\t\t\tep_reward += reward\n",
    "\t\t\tagent.replay.add(state, action, reward, next_state, done)\n",
    "\t\t\tstate = next_state\n",
    "\t\t\tsteps += 1\n",
    "\n",
    "\t\t\t# Train after warmup with proper episode tracking\n",
    "\t\t\tif agent.replay.size > max(config.batch_size, warmup_steps):\n",
    "\t\t\t\tloss = agent.train_on_batch(episode=ep, max_episodes=episodes)\n",
    "\n",
    "\t\t# Adaptive epsilon decay based on performance\n",
    "\t\tif len(recent_wins) >= 10 and np.mean(recent_wins) > 0.1:\n",
    "\t\t\t# Faster decay if we're learning\n",
    "\t\t\tfrac = (ep + 1) / max(1, config.epsilon_decay_episodes * 0.7)\n",
    "\t\telse:\n",
    "\t\t\t# Standard decay\n",
    "\t\t\tfrac = (ep + 1) / max(1, config.epsilon_decay_episodes)\n",
    "\t\t\n",
    "\t\tepsilon = exponential_epsilon(config.epsilon_start, config.epsilon_end, frac)\n",
    "\n",
    "\t\t# Enhanced metrics tracking\n",
    "\t\twin = float(env.revealed_safe >= env.num_safe_cells and env.done)\n",
    "\t\trecent_wins.append(win)\n",
    "\t\t\n",
    "\t\thistory[\"episode_reward\"].append(ep_reward)\n",
    "\t\thistory[\"episode_length\"].append(steps)\n",
    "\t\thistory[\"win\"].append(win)\n",
    "\t\thistory[\"epsilon\"].append(epsilon)\n",
    "\t\t\n",
    "\t\t# Compute moving averages\n",
    "\t\twin_rate_100 = np.mean(history[\"win\"][-100:])\n",
    "\t\twin_rate_50 = np.mean(history[\"win\"][-50:]) if len(history[\"win\"]) >= 50 else win_rate_100\n",
    "\t\t\n",
    "\t\thistory[\"win_rate\"].append(win_rate_100)\n",
    "\t\thistory[\"win_rate_50\"].append(win_rate_50)\n",
    "\t\t\n",
    "\t\t# Track best performance\n",
    "\t\tif win_rate_50 > best_winrate:\n",
    "\t\t\tbest_winrate = win_rate_50\n",
    "\t\t\tconsecutive_good_episodes = 0\n",
    "\t\telse:\n",
    "\t\t\tconsecutive_good_episodes += 1\n",
    "\n",
    "\t\t# Progress reporting\n",
    "\t\tif (ep + 1) % show_progress_every == 0:\n",
    "\t\t\tavg_reward = np.mean(history[\"episode_reward\"][-show_progress_every:])\n",
    "\t\t\tavg_steps = np.mean(history[\"episode_length\"][-show_progress_every:])\n",
    "\t\t\tprint(f\"Episode {ep+1}/{episodes} | Reward {avg_reward:.1f} | Steps {avg_steps:.1f} | \"\n",
    "\t\t\t\t  f\"WinRate@50 {win_rate_50:.3f} | WinRate@100 {win_rate_100:.3f} | \"\n",
    "\t\t\t\t  f\"Eps {epsilon:.3f} | Mines {env.num_mines}\")\n",
    "\t\t\n",
    "\t\t# Early stopping if we achieve very good performance\n",
    "\t\tif win_rate_50 >= 0.8 and len(history[\"win\"]) >= 200:\n",
    "\t\t\tprint(f\"Early stopping: Achieved {win_rate_50:.3f} win rate!\")\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn agent, history\n",
    "\n",
    "\n",
    "# Evaluation utilities\n",
    "def evaluate_agent(env, agent, num_games=100, render_failures=False):\n",
    "\t\"\"\"Comprehensive agent evaluation\"\"\"\n",
    "\twins = 0\n",
    "\ttotal_steps = 0\n",
    "\ttotal_rewards = 0\n",
    "\tgame_lengths = []\n",
    "\t\n",
    "\tfor i in range(num_games):\n",
    "\t\tstate = env.reset()\n",
    "\t\tdone = False\n",
    "\t\tsteps = 0\n",
    "\t\tep_reward = 0\n",
    "\t\t\n",
    "\t\twhile not done and steps < env.height * env.width * 2:\n",
    "\t\t\tvalid_mask = env.valid_action_mask()\n",
    "\t\t\t# Greedy evaluation\n",
    "\t\t\tq = agent.online.predict(state[None, ...], verbose=0)[0]\n",
    "\t\t\tmasked_q = np.where(valid_mask > 0.5, q, -1e9)\n",
    "\t\t\taction = int(np.argmax(masked_q))\n",
    "\t\t\t\n",
    "\t\t\tstate, reward, done, _ = env.step(action)\n",
    "\t\t\tep_reward += reward\n",
    "\t\t\tsteps += 1\n",
    "\t\t\n",
    "\t\twin = env.revealed_safe >= env.num_safe_cells and env.done\n",
    "\t\tif win:\n",
    "\t\t\twins += 1\n",
    "\t\telif render_failures and i < 5:  # Show first few failures\n",
    "\t\t\tprint(f\"Game {i+1} failed after {steps} steps (reward: {ep_reward:.1f})\")\n",
    "\t\t\tenv.render_ascii()\n",
    "\t\t\tprint()\n",
    "\t\t\n",
    "\t\ttotal_steps += steps\n",
    "\t\ttotal_rewards += ep_reward\n",
    "\t\tgame_lengths.append(steps)\n",
    "\t\n",
    "\twin_rate = wins / num_games\n",
    "\tavg_steps = total_steps / num_games\n",
    "\tavg_reward = total_rewards / num_games\n",
    "\t\n",
    "\tprint(f\"Evaluation over {num_games} games:\")\n",
    "\tprint(f\"Win Rate: {win_rate:.3f}\")\n",
    "\tprint(f\"Average Steps: {avg_steps:.1f}\")\n",
    "\tprint(f\"Average Reward: {avg_reward:.1f}\")\n",
    "\tprint(f\"Step Distribution: min={min(game_lengths)}, max={max(game_lengths)}, std={np.std(game_lengths):.1f}\")\n",
    "\t\n",
    "\treturn win_rate, avg_steps, avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Enhanced Configuration\n",
    "\n",
    "HEIGHT = 8\n",
    "WIDTH = 8\n",
    "MINES = 10\n",
    "EPISODES = 2500  # Increased for better convergence\n",
    "\n",
    "print(f\"Training on {HEIGHT}x{WIDTH} board with {MINES} mines\")\n",
    "print(f\"Safe cells to reveal: {HEIGHT * WIDTH - MINES}\")\n",
    "\n",
    "# Create environment\n",
    "env = MinesweeperEnv(height=HEIGHT, width=WIDTH, num_mines=MINES, first_click_safe=True, seed=123)\n",
    "\n",
    "# Enhanced configuration\n",
    "config = DQNConfig(\n",
    "\tlearning_rate=3e-4,          # Optimized learning rate\n",
    "\tgamma=0.995,                 # Higher discount for long-term planning\n",
    "\ttarget_update_every=500,     # More frequent target updates\n",
    "\tbatch_size=128,              # Larger batch size\n",
    "\treplay_capacity=200_000,     # Much larger replay buffer\n",
    "\tepsilon_start=1.0,\n",
    "\tepsilon_end=0.02,            # Lower minimum epsilon\n",
    "\tepsilon_decay_episodes=1500, # Longer decay period\n",
    "\tconv_channels=64,            # More powerful network\n",
    "\tfc_units=256,\n",
    "\t# Prioritized replay parameters\n",
    "\talpha=0.6,\n",
    "\tbeta_start=0.4,\n",
    "\tbeta_end=1.0,\n",
    "\t# Regularization\n",
    "\tgradient_clip=10.0,\n",
    "\tweight_decay=1e-5,\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"- Network: {config.conv_channels} conv channels, {config.fc_units} FC units\")\n",
    "print(f\"- Learning rate: {config.learning_rate}\")\n",
    "print(f\"- Replay buffer: {config.replay_capacity:,} experiences\")\n",
    "print(f\"- Prioritized replay: α={config.alpha}, β={config.beta_start}→{config.beta_end}\")\n",
    "print(f\"- Episodes: {EPISODES} with curriculum learning\")\n",
    "print()\n",
    "\n",
    "# Train the enhanced agent\n",
    "agent, history = train_enhanced_agent(\n",
    "\tenv,\n",
    "\tepisodes=EPISODES,\n",
    "\twarmup_steps=2000,           # Longer warmup for better exploration\n",
    "\tmax_steps_per_episode=None,\n",
    "\tconfig=config,\n",
    "\tshow_progress_every=100,\n",
    "\tcurriculum_learning=True,    # Enable curriculum learning\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final win rate (last 100): {np.mean(history['win'][-100:]):.3f}\")\n",
    "print(f\"Final win rate (last 50): {np.mean(history['win'][-50:]):.3f}\")\n",
    "print(f\"Best 50-episode win rate: {max(history['win_rate_50']):.3f}\")\n",
    "\n",
    "# Plot enhanced training curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "metrics = [\n",
    "\t(\"episode_reward\", \"Episode Reward\"),\n",
    "\t(\"win_rate\", \"Win Rate (100-ep avg)\"),\n",
    "\t(\"win_rate_50\", \"Win Rate (50-ep avg)\"),\n",
    "\t(\"episode_length\", \"Episode Length\"),\n",
    "\t(\"epsilon\", \"Epsilon\"),\n",
    "\t(\"win\", \"Individual Wins\")\n",
    "]\n",
    "\n",
    "for ax, (k, title) in zip(axes.ravel(), metrics):\n",
    "\tif k in history:\n",
    "\t\tif k == \"win\":\n",
    "\t\t\t# Show wins as scatter plot\n",
    "\t\t\twins = np.array(history[k])\n",
    "\t\t\twin_indices = np.where(wins == 1)[0]\n",
    "\t\t\tax.scatter(win_indices, wins[win_indices], alpha=0.6, s=10)\n",
    "\t\t\tax.set_ylim(-0.1, 1.1)\n",
    "\t\telse:\n",
    "\t\t\tax.plot(history[k])\n",
    "\t\tax.set_title(title)\n",
    "\t\tax.set_xlabel(\"Episode\")\n",
    "\t\tax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\nRunning final evaluation...\")\n",
    "final_win_rate, avg_steps, avg_reward = evaluate_agent(env, agent, num_games=200, render_failures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Evaluation and Gameplay Visualization\n",
    "\n",
    "def greedy_action_enhanced(agent, state, env):\n",
    "\t\"\"\"Enhanced action selection with safety consideration\"\"\"\n",
    "\tmask = env.valid_action_mask()\n",
    "\tq = agent.online.predict(state[None, ...], verbose=0)[0]\n",
    "\t\n",
    "\t# Add slight safety bias to Q-values\n",
    "\tsafety_scores = state[..., 11].flatten()\n",
    "\tsafety_bias = safety_scores * 0.05\n",
    "\t\n",
    "\tq_with_safety = np.where(mask > 0.5, q + safety_bias, -1e9)\n",
    "\taction = int(np.argmax(q_with_safety))\n",
    "\t\n",
    "\treturn action, q.reshape(env.height, env.width), safety_scores.reshape(env.height, env.width)\n",
    "\n",
    "\n",
    "def play_enhanced_game(env, agent, render_each=False, delay=0.0, collect_frames=False):\n",
    "\t\"\"\"Play a single game with enhanced tracking\"\"\"\n",
    "\tstate = env.reset()\n",
    "\tframes = []\n",
    "\tqmaps = []\n",
    "\tsafety_maps = []\n",
    "\tactions_taken = []\n",
    "\trewards_received = []\n",
    "\tsteps = 0\n",
    "\t\n",
    "\twhile not env.done and steps < env.height * env.width * 2:\n",
    "\t\taction, q_map, safety_map = greedy_action_enhanced(agent, state, env)\n",
    "\t\tr, c = divmod(action, env.width)\n",
    "\t\t\n",
    "\t\tstate, reward, done, _ = env.step(action)\n",
    "\t\t\n",
    "\t\tactions_taken.append((r, c))\n",
    "\t\trewards_received.append(reward)\n",
    "\t\t\n",
    "\t\tif collect_frames or render_each:\n",
    "\t\t\tframes.append(render_board_image(env))\n",
    "\t\t\tqmaps.append(q_map)\n",
    "\t\t\tsafety_maps.append(safety_map)\n",
    "\t\t\t\n",
    "\t\t\tif render_each:\n",
    "\t\t\t\tfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Board state\n",
    "\t\t\t\taxes[0].imshow(frames[-1])\n",
    "\t\t\t\taxes[0].set_title(f\"Step {steps+1}: Click ({r},{c})\")\n",
    "\t\t\t\taxes[0].plot(c, r, 'rx', markersize=10, markeredgewidth=2)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Q-values\n",
    "\t\t\t\tim1 = axes[1].imshow(qmaps[-1], cmap=\"viridis\")\n",
    "\t\t\t\taxes[1].set_title(f\"Q-Values (max: {qmaps[-1].max():.2f})\")\n",
    "\t\t\t\tplt.colorbar(im1, ax=axes[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Safety map\n",
    "\t\t\t\tim2 = axes[2].imshow(safety_maps[-1], cmap=\"RdYlGn\")\n",
    "\t\t\t\taxes[2].set_title(\"Safety Scores\")\n",
    "\t\t\t\tplt.colorbar(im2, ax=axes[2])\n",
    "\t\t\t\t\n",
    "\t\t\t\tplt.tight_layout()\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\ttime.sleep(delay)\n",
    "\t\t\n",
    "\t\tsteps += 1\n",
    "\t\n",
    "\twin = env.done and env.revealed_safe >= env.num_safe_cells\n",
    "\ttotal_reward = sum(rewards_received)\n",
    "\t\n",
    "\treturn win, steps, total_reward, frames, qmaps, safety_maps, actions_taken, rewards_received\n",
    "\n",
    "\n",
    "# Run detailed evaluation games\n",
    "print(\"Running detailed evaluation...\")\n",
    "N_EVAL = 50\n",
    "wins = 0\n",
    "all_steps = []\n",
    "all_rewards = []\n",
    "failure_reasons = []\n",
    "\n",
    "for i in range(N_EVAL):\n",
    "\twin, steps, total_reward, _, _, _, actions, rewards = play_enhanced_game(env, agent)\n",
    "\twins += int(win)\n",
    "\tall_steps.append(steps)\n",
    "\tall_rewards.append(total_reward)\n",
    "\t\n",
    "\tif not win:\n",
    "\t\t# Analyze failure\n",
    "\t\tif env.revealed_safe == 0:\n",
    "\t\t\tfailure_reasons.append(\"First click mine\")\n",
    "\t\telif env.revealed_safe < env.num_safe_cells * 0.3:\n",
    "\t\t\tfailure_reasons.append(\"Early game failure\")\n",
    "\t\telif env.revealed_safe < env.num_safe_cells * 0.7:\n",
    "\t\t\tfailure_reasons.append(\"Mid game failure\")\n",
    "\t\telse:\n",
    "\t\t\tfailure_reasons.append(\"Late game failure\")\n",
    "\n",
    "print(f\"\\nDetailed Evaluation Results ({N_EVAL} games):\")\n",
    "print(f\"Win rate: {wins/N_EVAL:.3f} ({wins}/{N_EVAL})\")\n",
    "print(f\"Average steps: {np.mean(all_steps):.1f} ± {np.std(all_steps):.1f}\")\n",
    "print(f\"Average reward: {np.mean(all_rewards):.1f} ± {np.std(all_rewards):.1f}\")\n",
    "\n",
    "if failure_reasons:\n",
    "\tfrom collections import Counter\n",
    "\tfailure_counts = Counter(failure_reasons)\n",
    "\tprint(f\"\\nFailure analysis:\")\n",
    "\tfor reason, count in failure_counts.items():\n",
    "\t\tprint(f\"  {reason}: {count}/{len(failure_reasons)} ({count/len(failure_reasons)*100:.1f}%)\")\n",
    "\n",
    "# Show example successful game\n",
    "print(f\"\\nShowing example game (interactive visualization):\")\n",
    "win, steps, total_reward, frames, qmaps, safety_maps, actions, rewards = play_enhanced_game(\n",
    "\tenv, agent, collect_frames=True\n",
    ")\n",
    "print(f\"Example game: Win={win}, Steps={steps}, Total Reward={total_reward:.1f}\")\n",
    "\n",
    "if len(frames) > 0:\n",
    "\tprint(\"Game successful - showing first few steps...\")\n",
    "\tfor i in range(min(3, len(frames))):\n",
    "\t\tr, c = actions[i]\n",
    "\t\treward = rewards[i]\n",
    "\t\t\n",
    "\t\tfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\t\t\n",
    "\t\t# Board\n",
    "\t\taxes[0].imshow(frames[i])\n",
    "\t\taxes[0].set_title(f\"Step {i+1}: Click ({r},{c}), Reward: {reward:.2f}\")\n",
    "\t\taxes[0].plot(c, r, 'rx', markersize=12, markeredgewidth=3)\n",
    "\t\t\n",
    "\t\t# Q-values\n",
    "\t\tim1 = axes[1].imshow(qmaps[i], cmap=\"viridis\")\n",
    "\t\taxes[1].set_title(f\"Q-Values (range: {qmaps[i].min():.1f} to {qmaps[i].max():.1f})\")\n",
    "\t\tplt.colorbar(im1, ax=axes[1], shrink=0.6)\n",
    "\t\t\n",
    "\t\t# Safety\n",
    "\t\tim2 = axes[2].imshow(safety_maps[i], cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "\t\taxes[2].set_title(\"Safety Map\")\n",
    "\t\tplt.colorbar(im2, ax=axes[2], shrink=0.6)\n",
    "\t\t\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra visualizations: distributions and moving averages\n",
    "\n",
    "# Rewards distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(history[\"episode_reward\"], bins=30, kde=True)\n",
    "plt.title(\"Episode Reward Distribution\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "# Episode length distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(history[\"episode_length\"], bins=30, kde=True)\n",
    "plt.title(\"Episode Length Distribution\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.show()\n",
    "\n",
    "# Moving average of rewards\n",
    "window = min(100, max(10, len(history[\"episode_reward\"]) // 10))\n",
    "ma = np.convolve(np.array(history[\"episode_reward\"], dtype=np.float32), np.ones(window)/window, mode='valid')\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(ma)\n",
    "plt.title(f\"Reward Moving Average (window={window})\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"MA Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and Tips\n",
    "\n",
    "- Increase `EPISODES` to 2000–5000 for stronger play. Training time grows accordingly.\n",
    "- You can scale to larger boards (e.g., 12x12, 16x16) by updating `HEIGHT`, `WIDTH`, and `MINES`. Increase network capacity if needed.\n",
    "- The environment uses first-click safety and shaped rewards to help learning.\n",
    "- The Q heatmap shows agent preferences across unrevealed cells at each step.\n",
    "- Try curriculum training: start with fewer mines or smaller boards, then scale up.\n",
    "- For stability: consider prioritized replay, n-step returns, or a smaller learning rate.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}